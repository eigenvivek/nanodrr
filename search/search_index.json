{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"nanodrr","text":"<p>A performance-oriented reimplementation of <code>DiffDRR</code> with the following improvements:</p> <ul> <li>Optimized, pure PyTorch implementation (~5\u00d7 faster than <code>DiffDRR</code> at baseline)</li> <li>Modular design (freely swap subjects, extrinsics, and intrinsics during rendering)</li> <li>Compatibility with <code>torch.compile</code> and mixed precision</li> <li>Extensive type hints with <code>jaxtyping</code></li> <li>Standard Python package structure managed with <code>uv</code></li> </ul> <p>All projective geometry is implemented internally using the standard Hartley and Zisserman pinhole camera formulation.</p>"},{"location":"#installation","title":"Installation","text":"<p>Note</p> <p>On <code>pytorch&lt;2.9</code>, <code>torch.compile</code> with <code>bfloat16</code> is slower than eager due to a CUDA graph capture issue (see Benchmarks). Use <code>pytorch&gt;=2.9</code> (Triton \u22653.5) for best results.</p> <pre><code>pip install \"nanodrr[all]\"\n</code></pre>"},{"location":"#benchmarks","title":"Benchmarks","text":"<p>Important</p> <ul> <li>~5\u00d7 faster than <code>DiffDRR</code> out of the box, without compilation (946 FPS vs 213 FPS)</li> <li>~8\u00d7 faster with <code>torch.compile</code> and <code>bfloat16</code> on <code>pytorch&gt;=2.9</code> (1,650 FPS vs 213 FPS)</li> <li>~2.5\u00d7 less memory than <code>DiffDRR</code> (516 MB vs 1,344 MB peak reserved with <code>bfloat16</code> + compile)</li> </ul> <p></p> <p>Mean \u00b1 std. dev. of 10 runs, 100 loops each. Benchmarked by rendering 200\u00d7200 DRRs on an NVIDIA RTX 6000 Ada (48 GB) with Python 3.12. Compile represents <code>torch.compile(mode=\"reduce-overhead\", fullgraph=True)</code>. Full experiment at <code>tests/benchmark/</code>.</p>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>[x] Implement a fully optimized renderer</li> <li>[x] Port strictly necessary modules from <code>DiffDRR</code> (e.g., SE(3) utilities, loss functions, and 2D plotting)</li> <li>[x] Migrate 3D plotting functions to an optional module</li> <li>[ ] Integrate with <code>xvr</code> to speed up network training and registration</li> <li>[ ] Integrate with <code>polypose</code> to speed up registration</li> <li>[ ] Release as <code>v1.0.0</code> of <code>DiffDRR</code>!</li> </ul>"},{"location":"docs/tutorials/demo/","title":"Demonstration","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport torch\n\nfrom nanodrr.camera import make_k_inv, make_rt_inv\nfrom nanodrr.data import Subject, download_deepfluoro\nfrom nanodrr.plot import plot_drr\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n</pre> import matplotlib.pyplot as plt import torch  from nanodrr.camera import make_k_inv, make_rt_inv from nanodrr.data import Subject, download_deepfluoro from nanodrr.plot import plot_drr  device = \"cuda\" if torch.cuda.is_available() else \"cpu\" <pre>/home/runner/work/nanodrr/nanodrr/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre># Load the volume and (optional) segmentation\nimagepath, labelpath = download_deepfluoro(subject=1)\nsubject = Subject.from_filepath(imagepath, labelpath).to(device)\n</pre> # Load the volume and (optional) segmentation imagepath, labelpath = download_deepfluoro(subject=1) subject = Subject.from_filepath(imagepath, labelpath).to(device) <p>Additionally, all coordinate transforms needed for rendering (camera \u2192 world \u2192 voxel \u2192 grid sample) are fused, which dramatically speeds up rendering.</p> In\u00a0[3]: Copied! <pre>from nanodrr.drr import render\n</pre> from nanodrr.drr import render In\u00a0[4]: Copied! <pre># Construct the inverse intrinsic matrix from C-arm imaging parameters\nsdd = 1020.0\ndelx = dely = 2.0\nx0 = y0 = 0.0\nheight = width = 200\n\nk_inv = make_k_inv(sdd, delx, dely, x0, y0, height, width, device=device)\nsdd = torch.tensor([sdd], device=device)\n</pre> # Construct the inverse intrinsic matrix from C-arm imaging parameters sdd = 1020.0 delx = dely = 2.0 x0 = y0 = 0.0 height = width = 200  k_inv = make_k_inv(sdd, delx, dely, x0, y0, height, width, device=device) sdd = torch.tensor([sdd], device=device) <p>Although subject's are no longer centered at the origin in world coordinates, we can achieve the same camera parameterization as <code>DiffDRR</code> by constructing the C-arm pose relative to the subject's isocenter.</p> In\u00a0[5]: Copied! <pre># Construct the inverse extrinsic matrix\nrt_inv = make_rt_inv(\n    torch.tensor(\n        [\n            [0.0, 0.0, 0.0],\n            [30.0, 0.0, 0.0],\n        ],\n        device=device,\n    ),\n    torch.tensor(\n        [\n            [0.0, 850.0, 0.0],\n            [0.0, 850.0, 0.0],\n        ],\n        device=device,\n    ),\n    orientation=\"AP\",\n    isocenter=subject.isocenter,  # Move to the subject's isocenter\n)\n</pre> # Construct the inverse extrinsic matrix rt_inv = make_rt_inv(     torch.tensor(         [             [0.0, 0.0, 0.0],             [30.0, 0.0, 0.0],         ],         device=device,     ),     torch.tensor(         [             [0.0, 850.0, 0.0],             [0.0, 850.0, 0.0],         ],         device=device,     ),     orientation=\"AP\",     isocenter=subject.isocenter,  # Move to the subject's isocenter ) <p>In <code>DiffDRR</code>, we were constrined to rendering X-rays from a single subject with fixed intrinsics. With the functional interface in <code>nanodrr</code>, we can freely exchange the subject and intrinsics, as well as the C-arm pose.</p> <p>Also, note that the intrinsic matrix can be different for every image being rendered, which is useful for biplane C-arm setups.</p> In\u00a0[6]: Copied! <pre># Render the DRR\nimg = render(subject, k_inv, rt_inv, sdd, height, width)\nprint(img.shape)\n</pre> # Render the DRR img = render(subject, k_inv, rt_inv, sdd, height, width) print(img.shape) <pre>torch.Size([2, 8, 200, 200])\n</pre> <p>As in <code>DiffDRR</code>, if our <code>Subject</code> has a labelmap, the rendered DRRs are multichannel with each channel corresponding to a distinct anatomical structure.</p> In\u00a0[7]: Copied! <pre># Plot the DRR and overlay the projected segmentation labels\nplot_drr(img, ticks=False)\nplt.show()\n\n# Plot the DRR\nplot_drr(img.sum(dim=1, keepdim=True), ticks=False)\nplt.show()\n</pre> # Plot the DRR and overlay the projected segmentation labels plot_drr(img, ticks=False) plt.show()  # Plot the DRR plot_drr(img.sum(dim=1, keepdim=True), ticks=False) plt.show() In\u00a0[8]: Copied! <pre>from nanodrr.drr import DRR\n</pre> from nanodrr.drr import DRR In\u00a0[9]: Copied! <pre># Initialize the DRR module from the intrinsics\ndrr = DRR.from_carm_intrinsics(\n    sdd,\n    delx,\n    dely,\n    x0,\n    y0,\n    height,\n    width,\n    device=device,\n)\n\n# Render with fixed intrinsics\nimg = drr(subject, rt_inv)\nplot_drr(img)\nplt.show()\n</pre> # Initialize the DRR module from the intrinsics drr = DRR.from_carm_intrinsics(     sdd,     delx,     dely,     x0,     y0,     height,     width,     device=device, )  # Render with fixed intrinsics img = drr(subject, rt_inv) plot_drr(img) plt.show()"},{"location":"docs/tutorials/demo/#demonstration","title":"Demonstration\u00b6","text":"<p><code>nanodrr</code> features the same capabilities as <code>DiffDRR</code>, but with improved performance and interfaces.</p> <p>This tutorial highlights the important similarities and differences between <code>nanodrr</code> and <code>DiffDRR</code>:</p> <ul> <li>A new <code>Subject</code> class</li> <li>A functional DRR rendering interface</li> <li>A new internal camera geometry based on pinhole cameras</li> <li>The ability to freely exchange subjects, intrinsics, and extrinsics at runtime</li> </ul> <p>Another fun change is that, with <code>pytorch&gt;=2.10</code>, rendering without a labelmap works on MPS!</p>"},{"location":"docs/tutorials/demo/#subject","title":"Subject\u00b6","text":"<p><code>nanodrr</code> also uses a <code>Subject</code> to hold a volume, (optional) labelmap, and coordinate frame transforms. However, now volumes are not centered at the origin by default.</p>"},{"location":"docs/tutorials/demo/#functional-interface","title":"Functional interface\u00b6","text":"<p><code>nanodrr</code> provides a functional interface for DRR rendering (<code>nanodrr.drr.render</code>).</p> <p>The inputs are:</p> <ul> <li>A <code>Subject</code> object</li> <li>An (batched) inverse intrinsic matrix</li> <li>An (batched) inverse extrinsic matrix</li> </ul> <p>Intrinsic and extrinsic matrices come from the pinhole camera model. Utilities are provided to construct these matrices from standard C-arm imaging parameters.</p>"},{"location":"docs/tutorials/demo/#class-interface","title":"Class interface\u00b6","text":"<p>The traditional class interface from <code>DiffDRR</code> is also available if you wish to have fixed intrinsics. However, note that the new class interface also let's you freely change the user at runtime.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>camera<ul> <li>extrinsics</li> <li>homography</li> <li>intrinsics</li> </ul> </li> <li>data<ul> <li>demo</li> <li>io</li> <li>preprocess</li> </ul> </li> <li>drr<ul> <li>drr</li> <li>render</li> </ul> </li> <li>geometry<ul> <li>se3</li> <li>transform</li> </ul> </li> <li>metrics<ul> <li>geo</li> <li>ncc</li> </ul> </li> <li>plot<ul> <li>plot</li> </ul> </li> <li>registration<ul> <li>registration</li> </ul> </li> <li>scene<ul> <li>camera</li> <li>scene</li> <li>surface</li> <li>utils</li> </ul> </li> </ul>"},{"location":"reference/nanodrr/camera/","title":"camera","text":""},{"location":"reference/nanodrr/camera/#nanodrr.camera","title":"nanodrr.camera","text":""},{"location":"reference/nanodrr/camera/#nanodrr.camera.resample","title":"resample","text":"<pre><code>resample(\n    img: Float[Tensor, \"B C H W\"],\n    k_inv_old: Float[Tensor, \"B 3 3\"],\n    k_inv_new: Float[Tensor, \"B 3 3\"],\n) -&gt; Float[Tensor, \"B C H W\"]\n</code></pre> <p>Resample an image from one camera's (inverse) intrinsic to another's.</p> <p>Each target pixel \\(p'\\) is mapped back to a source pixel via the homography</p> \\[ p = H p' = K_{\\text{old}} K_{\\text{new}}^{-1} p' \\] <p>and bilinearly interpolated. Out-of-bounds regions are filled with zeros.</p> PARAMETER DESCRIPTION <code>img</code> <p>Batch of images.</p> <p> TYPE: <code>Float[Tensor, 'B C H W']</code> </p> <code>k_inv_old</code> <p>Inverse intrinsic matrices of the source images.</p> <p> TYPE: <code>Float[Tensor, 'B 3 3']</code> </p> <code>k_inv_new</code> <p>Inverse intrinsic matrices of the target images.</p> <p> TYPE: <code>Float[Tensor, 'B 3 3']</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, 'B C H W']</code> <p>Resampled images.</p> Source code in <code>src/nanodrr/camera/homography.py</code> <pre><code>def resample(\n    img: Float[torch.Tensor, \"B C H W\"],\n    k_inv_old: Float[torch.Tensor, \"B 3 3\"],\n    k_inv_new: Float[torch.Tensor, \"B 3 3\"],\n) -&gt; Float[torch.Tensor, \"B C H W\"]:\n    r\"\"\"Resample an image from one camera's (inverse) intrinsic to another's.\n\n    Each target pixel \\(p'\\) is mapped back to a source pixel via the homography\n\n    \\[ p = H p' = K_{\\text{old}} K_{\\text{new}}^{-1} p' \\]\n\n    and bilinearly interpolated. Out-of-bounds regions are filled with zeros.\n\n    Args:\n        img: Batch of images.\n        k_inv_old: Inverse intrinsic matrices of the source images.\n        k_inv_new: Inverse intrinsic matrices of the target images.\n\n    Returns:\n        Resampled images.\n    \"\"\"\n    B, _, H, W = img.shape\n\n    # Destination -&gt; source homography per batch element\n    H_mat = torch.linalg.inv(k_inv_old) @ k_inv_new\n\n    # Build (H, W) grid of homogeneous destination pixel coords\n    ys, xs = torch.meshgrid(\n        torch.arange(H, dtype=img.dtype, device=img.device),\n        torch.arange(W, dtype=img.dtype, device=img.device),\n        indexing=\"ij\",\n    )\n    ones = torch.ones_like(xs)\n    coords = torch.stack([xs, ys, ones], dim=-1)\n\n    # Apply per-batch homography\n    coords_flat = coords.reshape(-1, 3).T.unsqueeze(0).expand(B, -1, -1)\n    src = (H_mat @ coords_flat).permute(0, 2, 1).reshape(B, H, W, 3)\n\n    # Perspective divide\n    src_x = src[..., 0] / src[..., 2]\n    src_y = src[..., 1] / src[..., 2]\n\n    # Normalize to [-1, 1] for grid_sample\n    grid_x = (src_x / (W - 1)) * 2 - 1\n    grid_y = (src_y / (H - 1)) * 2 - 1\n    grid = torch.stack([grid_x, grid_y], dim=-1)\n\n    return F.grid_sample(img, grid, align_corners=True, padding_mode=\"zeros\", mode=\"bilinear\")\n</code></pre>"},{"location":"reference/nanodrr/camera/#nanodrr.camera.make_k_inv","title":"make_k_inv","text":"<pre><code>make_k_inv(\n    sdd: float,\n    delx: float,\n    dely: float,\n    x0: float,\n    y0: float,\n    height: int,\n    width: int,\n    dtype: dtype | None = None,\n    device: device | None = None,\n) -&gt; Float[Tensor, \"1 3 3\"]\n</code></pre> <p>Build the inverse intrinsic matrix K\u207b\u00b9 for a cone-beam projector.</p> <p>Focal lengths and principal point are derived from the physical geometry:</p> <pre><code>fx = sdd / delx          cy = y0 / dely + height / 2\nfy = sdd / dely          cx = x0 / delx + width  / 2\n</code></pre> <p>The returned matrix is the analytical inverse of:</p> <pre><code>K = [[fx, 0, cx],\n     [0, fy, cy],\n     [0,  0,  1]]\n</code></pre> PARAMETER DESCRIPTION <code>sdd</code> <p>Source-to-detector distance (mm).</p> <p> TYPE: <code>float</code> </p> <code>delx</code> <p>Pixel spacing in x (mm/px).</p> <p> TYPE: <code>float</code> </p> <code>dely</code> <p>Pixel spacing in y (mm/px).</p> <p> TYPE: <code>float</code> </p> <code>x0</code> <p>Principal-point offset from detector centre in x (mm).</p> <p> TYPE: <code>float</code> </p> <code>y0</code> <p>Principal-point offset from detector centre in y (mm).</p> <p> TYPE: <code>float</code> </p> <code>height</code> <p>Detector height in pixels.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Detector width in pixels.</p> <p> TYPE: <code>int</code> </p> <code>dtype</code> <p>Optional tensor dtype.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Optional tensor device.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, '1 3 3']</code> <p>(1, 3, 3) inverse intrinsic matrix.</p> Source code in <code>src/nanodrr/camera/intrinsics.py</code> <pre><code>def make_k_inv(\n    sdd: float,\n    delx: float,\n    dely: float,\n    x0: float,\n    y0: float,\n    height: int,\n    width: int,\n    dtype: torch.dtype | None = None,\n    device: torch.device | None = None,\n) -&gt; Float[torch.Tensor, \"1 3 3\"]:\n    \"\"\"Build the inverse intrinsic matrix K\u207b\u00b9 for a cone-beam projector.\n\n    Focal lengths and principal point are derived from the physical geometry:\n\n        fx = sdd / delx          cy = y0 / dely + height / 2\n        fy = sdd / dely          cx = x0 / delx + width  / 2\n\n    The returned matrix is the analytical inverse of:\n\n        K = [[fx, 0, cx],\n             [0, fy, cy],\n             [0,  0,  1]]\n\n    Args:\n        sdd: Source-to-detector distance (mm).\n        delx: Pixel spacing in x (mm/px).\n        dely: Pixel spacing in y (mm/px).\n        x0: Principal-point offset from detector centre in x (mm).\n        y0: Principal-point offset from detector centre in y (mm).\n        height: Detector height in pixels.\n        width: Detector width in pixels.\n        dtype: Optional tensor dtype.\n        device: Optional tensor device.\n\n    Returns:\n        (1, 3, 3) inverse intrinsic matrix.\n    \"\"\"\n    fx = sdd / delx\n    fy = sdd / dely\n    cx = x0 / delx + width / 2.0\n    cy = y0 / dely + height / 2.0\n\n    return torch.tensor(\n        [\n            [\n                [1.0 / fx, 0.0, -cx / fx],\n                [0.0, 1.0 / fy, -cy / fy],\n                [0.0, 0.0, 1.0],\n            ]\n        ],\n        dtype=dtype,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/nanodrr/camera/#nanodrr.camera.make_rt_inv","title":"make_rt_inv","text":"<pre><code>make_rt_inv(\n    rotation: Float[Tensor, \"B 3\"],\n    translation: Float[Tensor, \"B 3\"],\n    orientation: str | None = \"AP\",\n    isocenter: Float[Tensor, 3] | None = None,\n) -&gt; Float[Tensor, \"B 4 4\"]\n</code></pre> <p>Create 4x4 camera-to-world (extrinsic inverse) matrices.</p> <p>Composes pose and reorientation as <code>extrinsic_inv = pose @ reorient</code> so that translation is applied in the pre-reoriented frame.</p> PARAMETER DESCRIPTION <code>rotation</code> <p>(B, 3) Euler angles (z, x, y) in degrees, ZXY convention.</p> <p> TYPE: <code>Float[Tensor, 'B 3']</code> </p> <code>translation</code> <p>(B, 3) camera position in mm, relative to isocenter          (or world origin when isocenter is <code>None</code>).</p> <p> TYPE: <code>Float[Tensor, 'B 3']</code> </p> <code>orientation</code> <p><code>\"AP\"</code>, <code>\"PA\"</code>, or <code>None</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'AP'</code> </p> <code>isocenter</code> <p>Optional (3,) volume centre in world coordinates.</p> <p> TYPE: <code>Float[Tensor, 3] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, 'B 4 4']</code> <p>(B, 4, 4) camera-to-world transformation matrices.</p> Source code in <code>src/nanodrr/camera/extrinsics.py</code> <pre><code>def make_rt_inv(\n    rotation: Float[torch.Tensor, \"B 3\"],\n    translation: Float[torch.Tensor, \"B 3\"],\n    orientation: str | None = \"AP\",\n    isocenter: Float[torch.Tensor, \"3\"] | None = None,\n) -&gt; Float[torch.Tensor, \"B 4 4\"]:\n    \"\"\"Create 4x4 camera-to-world (extrinsic inverse) matrices.\n\n    Composes pose and reorientation as ``extrinsic_inv = pose @ reorient``\n    so that *translation* is applied in the pre-reoriented frame.\n\n    Args:\n        rotation: (B, 3) Euler angles (z, x, y) in degrees, ZXY convention.\n        translation: (B, 3) camera position in mm, relative to *isocenter*\n                     (or world origin when isocenter is ``None``).\n        orientation: ``\"AP\"``, ``\"PA\"``, or ``None``.\n        isocenter: Optional (3,) volume centre in world coordinates.\n\n    Returns:\n        (B, 4, 4) camera-to-world transformation matrices.\n    \"\"\"\n    if orientation not in _ORIENTATION_MATRICES:\n        raise ValueError(f\"Unknown orientation: {orientation}. Use 'AP', 'PA', or None\")\n\n    device = rotation.device\n    dtype = rotation.dtype\n\n    if isocenter is None:\n        isocenter = torch.zeros(3, device=device, dtype=dtype)\n\n    pose = convert(rotation, translation, \"euler\", convention=\"ZXY\", isocenter=isocenter)\n    orientation_matrix = _get_orientation_matrix(orientation, device, dtype)\n\n    return pose @ orientation_matrix\n</code></pre>"},{"location":"reference/nanodrr/camera/extrinsics/","title":"extrinsics","text":""},{"location":"reference/nanodrr/camera/extrinsics/#nanodrr.camera.extrinsics","title":"nanodrr.camera.extrinsics","text":""},{"location":"reference/nanodrr/camera/extrinsics/#nanodrr.camera.extrinsics.make_rt_inv","title":"make_rt_inv","text":"<pre><code>make_rt_inv(\n    rotation: Float[Tensor, \"B 3\"],\n    translation: Float[Tensor, \"B 3\"],\n    orientation: str | None = \"AP\",\n    isocenter: Float[Tensor, 3] | None = None,\n) -&gt; Float[Tensor, \"B 4 4\"]\n</code></pre> <p>Create 4x4 camera-to-world (extrinsic inverse) matrices.</p> <p>Composes pose and reorientation as <code>extrinsic_inv = pose @ reorient</code> so that translation is applied in the pre-reoriented frame.</p> PARAMETER DESCRIPTION <code>rotation</code> <p>(B, 3) Euler angles (z, x, y) in degrees, ZXY convention.</p> <p> TYPE: <code>Float[Tensor, 'B 3']</code> </p> <code>translation</code> <p>(B, 3) camera position in mm, relative to isocenter          (or world origin when isocenter is <code>None</code>).</p> <p> TYPE: <code>Float[Tensor, 'B 3']</code> </p> <code>orientation</code> <p><code>\"AP\"</code>, <code>\"PA\"</code>, or <code>None</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'AP'</code> </p> <code>isocenter</code> <p>Optional (3,) volume centre in world coordinates.</p> <p> TYPE: <code>Float[Tensor, 3] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, 'B 4 4']</code> <p>(B, 4, 4) camera-to-world transformation matrices.</p> Source code in <code>src/nanodrr/camera/extrinsics.py</code> <pre><code>def make_rt_inv(\n    rotation: Float[torch.Tensor, \"B 3\"],\n    translation: Float[torch.Tensor, \"B 3\"],\n    orientation: str | None = \"AP\",\n    isocenter: Float[torch.Tensor, \"3\"] | None = None,\n) -&gt; Float[torch.Tensor, \"B 4 4\"]:\n    \"\"\"Create 4x4 camera-to-world (extrinsic inverse) matrices.\n\n    Composes pose and reorientation as ``extrinsic_inv = pose @ reorient``\n    so that *translation* is applied in the pre-reoriented frame.\n\n    Args:\n        rotation: (B, 3) Euler angles (z, x, y) in degrees, ZXY convention.\n        translation: (B, 3) camera position in mm, relative to *isocenter*\n                     (or world origin when isocenter is ``None``).\n        orientation: ``\"AP\"``, ``\"PA\"``, or ``None``.\n        isocenter: Optional (3,) volume centre in world coordinates.\n\n    Returns:\n        (B, 4, 4) camera-to-world transformation matrices.\n    \"\"\"\n    if orientation not in _ORIENTATION_MATRICES:\n        raise ValueError(f\"Unknown orientation: {orientation}. Use 'AP', 'PA', or None\")\n\n    device = rotation.device\n    dtype = rotation.dtype\n\n    if isocenter is None:\n        isocenter = torch.zeros(3, device=device, dtype=dtype)\n\n    pose = convert(rotation, translation, \"euler\", convention=\"ZXY\", isocenter=isocenter)\n    orientation_matrix = _get_orientation_matrix(orientation, device, dtype)\n\n    return pose @ orientation_matrix\n</code></pre>"},{"location":"reference/nanodrr/camera/homography/","title":"homography","text":""},{"location":"reference/nanodrr/camera/homography/#nanodrr.camera.homography","title":"nanodrr.camera.homography","text":""},{"location":"reference/nanodrr/camera/homography/#nanodrr.camera.homography.resample","title":"resample","text":"<pre><code>resample(\n    img: Float[Tensor, \"B C H W\"],\n    k_inv_old: Float[Tensor, \"B 3 3\"],\n    k_inv_new: Float[Tensor, \"B 3 3\"],\n) -&gt; Float[Tensor, \"B C H W\"]\n</code></pre> <p>Resample an image from one camera's (inverse) intrinsic to another's.</p> <p>Each target pixel \\(p'\\) is mapped back to a source pixel via the homography</p> \\[ p = H p' = K_{\\text{old}} K_{\\text{new}}^{-1} p' \\] <p>and bilinearly interpolated. Out-of-bounds regions are filled with zeros.</p> PARAMETER DESCRIPTION <code>img</code> <p>Batch of images.</p> <p> TYPE: <code>Float[Tensor, 'B C H W']</code> </p> <code>k_inv_old</code> <p>Inverse intrinsic matrices of the source images.</p> <p> TYPE: <code>Float[Tensor, 'B 3 3']</code> </p> <code>k_inv_new</code> <p>Inverse intrinsic matrices of the target images.</p> <p> TYPE: <code>Float[Tensor, 'B 3 3']</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, 'B C H W']</code> <p>Resampled images.</p> Source code in <code>src/nanodrr/camera/homography.py</code> <pre><code>def resample(\n    img: Float[torch.Tensor, \"B C H W\"],\n    k_inv_old: Float[torch.Tensor, \"B 3 3\"],\n    k_inv_new: Float[torch.Tensor, \"B 3 3\"],\n) -&gt; Float[torch.Tensor, \"B C H W\"]:\n    r\"\"\"Resample an image from one camera's (inverse) intrinsic to another's.\n\n    Each target pixel \\(p'\\) is mapped back to a source pixel via the homography\n\n    \\[ p = H p' = K_{\\text{old}} K_{\\text{new}}^{-1} p' \\]\n\n    and bilinearly interpolated. Out-of-bounds regions are filled with zeros.\n\n    Args:\n        img: Batch of images.\n        k_inv_old: Inverse intrinsic matrices of the source images.\n        k_inv_new: Inverse intrinsic matrices of the target images.\n\n    Returns:\n        Resampled images.\n    \"\"\"\n    B, _, H, W = img.shape\n\n    # Destination -&gt; source homography per batch element\n    H_mat = torch.linalg.inv(k_inv_old) @ k_inv_new\n\n    # Build (H, W) grid of homogeneous destination pixel coords\n    ys, xs = torch.meshgrid(\n        torch.arange(H, dtype=img.dtype, device=img.device),\n        torch.arange(W, dtype=img.dtype, device=img.device),\n        indexing=\"ij\",\n    )\n    ones = torch.ones_like(xs)\n    coords = torch.stack([xs, ys, ones], dim=-1)\n\n    # Apply per-batch homography\n    coords_flat = coords.reshape(-1, 3).T.unsqueeze(0).expand(B, -1, -1)\n    src = (H_mat @ coords_flat).permute(0, 2, 1).reshape(B, H, W, 3)\n\n    # Perspective divide\n    src_x = src[..., 0] / src[..., 2]\n    src_y = src[..., 1] / src[..., 2]\n\n    # Normalize to [-1, 1] for grid_sample\n    grid_x = (src_x / (W - 1)) * 2 - 1\n    grid_y = (src_y / (H - 1)) * 2 - 1\n    grid = torch.stack([grid_x, grid_y], dim=-1)\n\n    return F.grid_sample(img, grid, align_corners=True, padding_mode=\"zeros\", mode=\"bilinear\")\n</code></pre>"},{"location":"reference/nanodrr/camera/intrinsics/","title":"intrinsics","text":""},{"location":"reference/nanodrr/camera/intrinsics/#nanodrr.camera.intrinsics","title":"nanodrr.camera.intrinsics","text":""},{"location":"reference/nanodrr/camera/intrinsics/#nanodrr.camera.intrinsics.make_k_inv","title":"make_k_inv","text":"<pre><code>make_k_inv(\n    sdd: float,\n    delx: float,\n    dely: float,\n    x0: float,\n    y0: float,\n    height: int,\n    width: int,\n    dtype: dtype | None = None,\n    device: device | None = None,\n) -&gt; Float[Tensor, \"1 3 3\"]\n</code></pre> <p>Build the inverse intrinsic matrix K\u207b\u00b9 for a cone-beam projector.</p> <p>Focal lengths and principal point are derived from the physical geometry:</p> <pre><code>fx = sdd / delx          cy = y0 / dely + height / 2\nfy = sdd / dely          cx = x0 / delx + width  / 2\n</code></pre> <p>The returned matrix is the analytical inverse of:</p> <pre><code>K = [[fx, 0, cx],\n     [0, fy, cy],\n     [0,  0,  1]]\n</code></pre> PARAMETER DESCRIPTION <code>sdd</code> <p>Source-to-detector distance (mm).</p> <p> TYPE: <code>float</code> </p> <code>delx</code> <p>Pixel spacing in x (mm/px).</p> <p> TYPE: <code>float</code> </p> <code>dely</code> <p>Pixel spacing in y (mm/px).</p> <p> TYPE: <code>float</code> </p> <code>x0</code> <p>Principal-point offset from detector centre in x (mm).</p> <p> TYPE: <code>float</code> </p> <code>y0</code> <p>Principal-point offset from detector centre in y (mm).</p> <p> TYPE: <code>float</code> </p> <code>height</code> <p>Detector height in pixels.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Detector width in pixels.</p> <p> TYPE: <code>int</code> </p> <code>dtype</code> <p>Optional tensor dtype.</p> <p> TYPE: <code>dtype | None</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Optional tensor device.</p> <p> TYPE: <code>device | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, '1 3 3']</code> <p>(1, 3, 3) inverse intrinsic matrix.</p> Source code in <code>src/nanodrr/camera/intrinsics.py</code> <pre><code>def make_k_inv(\n    sdd: float,\n    delx: float,\n    dely: float,\n    x0: float,\n    y0: float,\n    height: int,\n    width: int,\n    dtype: torch.dtype | None = None,\n    device: torch.device | None = None,\n) -&gt; Float[torch.Tensor, \"1 3 3\"]:\n    \"\"\"Build the inverse intrinsic matrix K\u207b\u00b9 for a cone-beam projector.\n\n    Focal lengths and principal point are derived from the physical geometry:\n\n        fx = sdd / delx          cy = y0 / dely + height / 2\n        fy = sdd / dely          cx = x0 / delx + width  / 2\n\n    The returned matrix is the analytical inverse of:\n\n        K = [[fx, 0, cx],\n             [0, fy, cy],\n             [0,  0,  1]]\n\n    Args:\n        sdd: Source-to-detector distance (mm).\n        delx: Pixel spacing in x (mm/px).\n        dely: Pixel spacing in y (mm/px).\n        x0: Principal-point offset from detector centre in x (mm).\n        y0: Principal-point offset from detector centre in y (mm).\n        height: Detector height in pixels.\n        width: Detector width in pixels.\n        dtype: Optional tensor dtype.\n        device: Optional tensor device.\n\n    Returns:\n        (1, 3, 3) inverse intrinsic matrix.\n    \"\"\"\n    fx = sdd / delx\n    fy = sdd / dely\n    cx = x0 / delx + width / 2.0\n    cy = y0 / dely + height / 2.0\n\n    return torch.tensor(\n        [\n            [\n                [1.0 / fx, 0.0, -cx / fx],\n                [0.0, 1.0 / fy, -cy / fy],\n                [0.0, 0.0, 1.0],\n            ]\n        ],\n        dtype=dtype,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/nanodrr/data/","title":"data","text":""},{"location":"reference/nanodrr/data/#nanodrr.data","title":"nanodrr.data","text":""},{"location":"reference/nanodrr/data/#nanodrr.data.Subject","title":"Subject","text":"<pre><code>Subject(\n    imagedata: Float[Tensor, \"1 1 D H W\"],\n    labeldata: Float[Tensor, \"1 1 D H W\"],\n    voxel_to_world: Float[Tensor, \"4 4\"],\n    world_to_voxel: Float[Tensor, \"4 4\"],\n    voxel_to_grid: Float[Tensor, \"4 4\"],\n    isocenter: Float[Tensor, 3],\n    max_label: int | None = None,\n)\n</code></pre> <p>CT volume and (optional) labelmap compatible with <code>grid_sample</code>.</p> <p>Fuses all spatial transforms for sampling (world \u2192 voxel \u2192 grid) so downstream rendering only needs a single matmul.</p> Source code in <code>src/nanodrr/data/io.py</code> <pre><code>def __init__(\n    self,\n    imagedata: Float[torch.Tensor, \"1 1 D H W\"],\n    labeldata: Float[torch.Tensor, \"1 1 D H W\"],\n    voxel_to_world: Float[torch.Tensor, \"4 4\"],\n    world_to_voxel: Float[torch.Tensor, \"4 4\"],\n    voxel_to_grid: Float[torch.Tensor, \"4 4\"],\n    isocenter: Float[torch.Tensor, \"3\"],\n    max_label: int | None = None,\n) -&gt; None:\n    super().__init__()\n    self.register_buffer(\"image\", imagedata)\n    self.register_buffer(\"label\", labeldata)\n    self.register_buffer(\"world_to_grid\", voxel_to_grid @ world_to_voxel)\n    self.register_buffer(\"isocenter\", isocenter)\n\n    self.register_buffer(\"voxel_to_world\", voxel_to_world)\n    self.register_buffer(\"world_to_voxel\", world_to_voxel)\n    self.register_buffer(\"voxel_to_grid\", voxel_to_grid)\n\n    if max_label is not None:\n        self.n_classes = int(max_label + 1)\n    else:\n        self.n_classes = int(self.label.max().item()) + 1\n</code></pre>"},{"location":"reference/nanodrr/data/#nanodrr.data.Subject.from_filepath","title":"from_filepath  <code>classmethod</code>","text":"<pre><code>from_filepath(\n    imagepath: str | Path,\n    labelpath: str | Path | None = None,\n    convert_to_mu: bool = True,\n    mu_water: float = 0.0192,\n    mu_bone: float = 0.0573,\n    hu_bone: float = 1000.0,\n    max_label: int | None = None,\n) -&gt; Subject\n</code></pre> <p>Load a subject from NIfTI (or any TorchIO-supported) file paths.</p> PARAMETER DESCRIPTION <code>imagepath</code> <p>Path to the CT volume.</p> <p> TYPE: <code>str | Path</code> </p> <code>labelpath</code> <p>Optional path to a label map.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>convert_to_mu</code> <p>Convert Hounsfield units to linear attenuation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>mu_water</code> <p>Linear attenuation coefficient of water (mm\u207b\u00b9).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0192</code> </p> <code>max_label</code> <p>Override the maximum label index. If provided, <code>n_classes</code> is set to <code>max_label + 1</code> instead of being inferred from the data.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/nanodrr/data/io.py</code> <pre><code>@classmethod\ndef from_filepath(\n    cls,\n    imagepath: str | Path,\n    labelpath: str | Path | None = None,\n    convert_to_mu: bool = True,\n    mu_water: float = 0.0192,\n    mu_bone: float = 0.0573,\n    hu_bone: float = 1000.0,\n    max_label: int | None = None,\n) -&gt; \"Subject\":\n    \"\"\"Load a subject from NIfTI (or any TorchIO-supported) file paths.\n\n    Args:\n        imagepath: Path to the CT volume.\n        labelpath: Optional path to a label map.\n        convert_to_mu: Convert Hounsfield units to linear attenuation.\n        mu_water: Linear attenuation coefficient of water (mm\u207b\u00b9).\n        max_label: Override the maximum label index. If provided, ``n_classes``\n            is set to ``max_label + 1`` instead of being inferred from the data.\n    \"\"\"\n    image = ScalarImage(imagepath)\n    label = LabelMap(labelpath) if labelpath is not None else None\n    return cls.from_images(image, label, convert_to_mu, mu_water, mu_bone, hu_bone, max_label)\n</code></pre>"},{"location":"reference/nanodrr/data/#nanodrr.data.Subject.from_images","title":"from_images  <code>classmethod</code>","text":"<pre><code>from_images(\n    image: ScalarImage,\n    label: LabelMap | None = None,\n    convert_to_mu: bool = True,\n    mu_water: float = 0.0192,\n    mu_bone: float = 0.0573,\n    hu_bone: float = 1000.0,\n    max_label: int | None = None,\n) -&gt; Subject\n</code></pre> <p>Construct a subject from TorchIO image objects.</p> PARAMETER DESCRIPTION <code>image</code> <p>CT volume as a <code>ScalarImage</code>.</p> <p> TYPE: <code>ScalarImage</code> </p> <code>label</code> <p>Optional segmentation as a <code>LabelMap</code>.</p> <p> TYPE: <code>LabelMap | None</code> DEFAULT: <code>None</code> </p> <code>convert_to_mu</code> <p>Convert Hounsfield units to linear attenuation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>mu_water</code> <p>Linear attenuation coefficient of water (mm\u207b\u00b9).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0192</code> </p> <code>max_label</code> <p>Override the maximum label index. If provided, <code>n_classes</code> is set to <code>max_label + 1</code> instead of being inferred from the data.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/nanodrr/data/io.py</code> <pre><code>@classmethod\ndef from_images(\n    cls,\n    image: ScalarImage,\n    label: LabelMap | None = None,\n    convert_to_mu: bool = True,\n    mu_water: float = 0.0192,\n    mu_bone: float = 0.0573,\n    hu_bone: float = 1000.0,\n    max_label: int | None = None,\n) -&gt; \"Subject\":\n    \"\"\"Construct a subject from TorchIO image objects.\n\n    Args:\n        image: CT volume as a ``ScalarImage``.\n        label: Optional segmentation as a ``LabelMap``.\n        convert_to_mu: Convert Hounsfield units to linear attenuation.\n        mu_water: Linear attenuation coefficient of water (mm\u207b\u00b9).\n        max_label: Override the maximum label index. If provided, ``n_classes``\n            is set to ``max_label + 1`` instead of being inferred from the data.\n    \"\"\"\n    # Affine: invert in float64 for numerical accuracy, then downcast\n    voxel_to_world_f64 = torch.from_numpy(image.affine).to(torch.float64)\n    voxel_to_world = voxel_to_world_f64.to(torch.float32)\n    world_to_voxel = voxel_to_world_f64.inverse().to(torch.float32)\n\n    # Image data\n    imagedata = cls._to_bcdhw(image.data).to(torch.float32)\n    if convert_to_mu:\n        imagedata = hu_to_mu(imagedata, mu_water, mu_bone, hu_bone)\n\n    # Label data\n    if label is not None:\n        labeldata = cls._to_bcdhw(label.data).to(torch.float32)\n    else:\n        labeldata = torch.zeros_like(imagedata)\n\n    isocenter = torch.tensor(image.get_center(), dtype=torch.float32)\n    voxel_to_grid = cls._make_voxel_to_grid(imagedata.shape)\n\n    return cls(\n        imagedata,\n        labeldata,\n        voxel_to_world,\n        world_to_voxel,\n        voxel_to_grid,\n        isocenter,\n        max_label,\n    )\n</code></pre>"},{"location":"reference/nanodrr/data/#nanodrr.data.download_deepfluoro","title":"download_deepfluoro","text":"<pre><code>download_deepfluoro(subject: int = 1) -&gt; tuple[str, str]\n</code></pre> <p>Download a subject from the DeepFluoro dataset.</p> Source code in <code>src/nanodrr/data/demo.py</code> <pre><code>def download_deepfluoro(subject: int = 1) -&gt; tuple[str, str]:\n    \"\"\"Download a subject from the DeepFluoro dataset.\"\"\"\n    subject = f\"subject{subject:02d}\"\n    base_url = f\"https://huggingface.co/datasets/eigenvivek/xvr-data/resolve/main/deepfluoro/{subject}\"\n    imagepath = os.path.join(CACHE_DIR, \"deepfluoro\", subject, \"volume.nii.gz\")\n    labelpath = os.path.join(CACHE_DIR, \"deepfluoro\", subject, \"mask.nii.gz\")\n\n    for url, local_path in [\n        (f\"{base_url}/volume.nii.gz\", imagepath),\n        (f\"{base_url}/mask.nii.gz\", labelpath),\n    ]:\n        if not os.path.exists(local_path):\n            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n            torch.hub.download_url_to_file(url, local_path)\n\n    return imagepath, labelpath\n</code></pre>"},{"location":"reference/nanodrr/data/demo/","title":"demo","text":""},{"location":"reference/nanodrr/data/demo/#nanodrr.data.demo","title":"nanodrr.data.demo","text":""},{"location":"reference/nanodrr/data/demo/#nanodrr.data.demo.download_deepfluoro","title":"download_deepfluoro","text":"<pre><code>download_deepfluoro(subject: int = 1) -&gt; tuple[str, str]\n</code></pre> <p>Download a subject from the DeepFluoro dataset.</p> Source code in <code>src/nanodrr/data/demo.py</code> <pre><code>def download_deepfluoro(subject: int = 1) -&gt; tuple[str, str]:\n    \"\"\"Download a subject from the DeepFluoro dataset.\"\"\"\n    subject = f\"subject{subject:02d}\"\n    base_url = f\"https://huggingface.co/datasets/eigenvivek/xvr-data/resolve/main/deepfluoro/{subject}\"\n    imagepath = os.path.join(CACHE_DIR, \"deepfluoro\", subject, \"volume.nii.gz\")\n    labelpath = os.path.join(CACHE_DIR, \"deepfluoro\", subject, \"mask.nii.gz\")\n\n    for url, local_path in [\n        (f\"{base_url}/volume.nii.gz\", imagepath),\n        (f\"{base_url}/mask.nii.gz\", labelpath),\n    ]:\n        if not os.path.exists(local_path):\n            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n            torch.hub.download_url_to_file(url, local_path)\n\n    return imagepath, labelpath\n</code></pre>"},{"location":"reference/nanodrr/data/io/","title":"io","text":""},{"location":"reference/nanodrr/data/io/#nanodrr.data.io","title":"nanodrr.data.io","text":""},{"location":"reference/nanodrr/data/io/#nanodrr.data.io.Subject","title":"Subject","text":"<pre><code>Subject(\n    imagedata: Float[Tensor, \"1 1 D H W\"],\n    labeldata: Float[Tensor, \"1 1 D H W\"],\n    voxel_to_world: Float[Tensor, \"4 4\"],\n    world_to_voxel: Float[Tensor, \"4 4\"],\n    voxel_to_grid: Float[Tensor, \"4 4\"],\n    isocenter: Float[Tensor, 3],\n    max_label: int | None = None,\n)\n</code></pre> <p>CT volume and (optional) labelmap compatible with <code>grid_sample</code>.</p> <p>Fuses all spatial transforms for sampling (world \u2192 voxel \u2192 grid) so downstream rendering only needs a single matmul.</p> Source code in <code>src/nanodrr/data/io.py</code> <pre><code>def __init__(\n    self,\n    imagedata: Float[torch.Tensor, \"1 1 D H W\"],\n    labeldata: Float[torch.Tensor, \"1 1 D H W\"],\n    voxel_to_world: Float[torch.Tensor, \"4 4\"],\n    world_to_voxel: Float[torch.Tensor, \"4 4\"],\n    voxel_to_grid: Float[torch.Tensor, \"4 4\"],\n    isocenter: Float[torch.Tensor, \"3\"],\n    max_label: int | None = None,\n) -&gt; None:\n    super().__init__()\n    self.register_buffer(\"image\", imagedata)\n    self.register_buffer(\"label\", labeldata)\n    self.register_buffer(\"world_to_grid\", voxel_to_grid @ world_to_voxel)\n    self.register_buffer(\"isocenter\", isocenter)\n\n    self.register_buffer(\"voxel_to_world\", voxel_to_world)\n    self.register_buffer(\"world_to_voxel\", world_to_voxel)\n    self.register_buffer(\"voxel_to_grid\", voxel_to_grid)\n\n    if max_label is not None:\n        self.n_classes = int(max_label + 1)\n    else:\n        self.n_classes = int(self.label.max().item()) + 1\n</code></pre>"},{"location":"reference/nanodrr/data/io/#nanodrr.data.io.Subject.from_filepath","title":"from_filepath  <code>classmethod</code>","text":"<pre><code>from_filepath(\n    imagepath: str | Path,\n    labelpath: str | Path | None = None,\n    convert_to_mu: bool = True,\n    mu_water: float = 0.0192,\n    mu_bone: float = 0.0573,\n    hu_bone: float = 1000.0,\n    max_label: int | None = None,\n) -&gt; Subject\n</code></pre> <p>Load a subject from NIfTI (or any TorchIO-supported) file paths.</p> PARAMETER DESCRIPTION <code>imagepath</code> <p>Path to the CT volume.</p> <p> TYPE: <code>str | Path</code> </p> <code>labelpath</code> <p>Optional path to a label map.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>convert_to_mu</code> <p>Convert Hounsfield units to linear attenuation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>mu_water</code> <p>Linear attenuation coefficient of water (mm\u207b\u00b9).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0192</code> </p> <code>max_label</code> <p>Override the maximum label index. If provided, <code>n_classes</code> is set to <code>max_label + 1</code> instead of being inferred from the data.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/nanodrr/data/io.py</code> <pre><code>@classmethod\ndef from_filepath(\n    cls,\n    imagepath: str | Path,\n    labelpath: str | Path | None = None,\n    convert_to_mu: bool = True,\n    mu_water: float = 0.0192,\n    mu_bone: float = 0.0573,\n    hu_bone: float = 1000.0,\n    max_label: int | None = None,\n) -&gt; \"Subject\":\n    \"\"\"Load a subject from NIfTI (or any TorchIO-supported) file paths.\n\n    Args:\n        imagepath: Path to the CT volume.\n        labelpath: Optional path to a label map.\n        convert_to_mu: Convert Hounsfield units to linear attenuation.\n        mu_water: Linear attenuation coefficient of water (mm\u207b\u00b9).\n        max_label: Override the maximum label index. If provided, ``n_classes``\n            is set to ``max_label + 1`` instead of being inferred from the data.\n    \"\"\"\n    image = ScalarImage(imagepath)\n    label = LabelMap(labelpath) if labelpath is not None else None\n    return cls.from_images(image, label, convert_to_mu, mu_water, mu_bone, hu_bone, max_label)\n</code></pre>"},{"location":"reference/nanodrr/data/io/#nanodrr.data.io.Subject.from_images","title":"from_images  <code>classmethod</code>","text":"<pre><code>from_images(\n    image: ScalarImage,\n    label: LabelMap | None = None,\n    convert_to_mu: bool = True,\n    mu_water: float = 0.0192,\n    mu_bone: float = 0.0573,\n    hu_bone: float = 1000.0,\n    max_label: int | None = None,\n) -&gt; Subject\n</code></pre> <p>Construct a subject from TorchIO image objects.</p> PARAMETER DESCRIPTION <code>image</code> <p>CT volume as a <code>ScalarImage</code>.</p> <p> TYPE: <code>ScalarImage</code> </p> <code>label</code> <p>Optional segmentation as a <code>LabelMap</code>.</p> <p> TYPE: <code>LabelMap | None</code> DEFAULT: <code>None</code> </p> <code>convert_to_mu</code> <p>Convert Hounsfield units to linear attenuation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>mu_water</code> <p>Linear attenuation coefficient of water (mm\u207b\u00b9).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0192</code> </p> <code>max_label</code> <p>Override the maximum label index. If provided, <code>n_classes</code> is set to <code>max_label + 1</code> instead of being inferred from the data.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/nanodrr/data/io.py</code> <pre><code>@classmethod\ndef from_images(\n    cls,\n    image: ScalarImage,\n    label: LabelMap | None = None,\n    convert_to_mu: bool = True,\n    mu_water: float = 0.0192,\n    mu_bone: float = 0.0573,\n    hu_bone: float = 1000.0,\n    max_label: int | None = None,\n) -&gt; \"Subject\":\n    \"\"\"Construct a subject from TorchIO image objects.\n\n    Args:\n        image: CT volume as a ``ScalarImage``.\n        label: Optional segmentation as a ``LabelMap``.\n        convert_to_mu: Convert Hounsfield units to linear attenuation.\n        mu_water: Linear attenuation coefficient of water (mm\u207b\u00b9).\n        max_label: Override the maximum label index. If provided, ``n_classes``\n            is set to ``max_label + 1`` instead of being inferred from the data.\n    \"\"\"\n    # Affine: invert in float64 for numerical accuracy, then downcast\n    voxel_to_world_f64 = torch.from_numpy(image.affine).to(torch.float64)\n    voxel_to_world = voxel_to_world_f64.to(torch.float32)\n    world_to_voxel = voxel_to_world_f64.inverse().to(torch.float32)\n\n    # Image data\n    imagedata = cls._to_bcdhw(image.data).to(torch.float32)\n    if convert_to_mu:\n        imagedata = hu_to_mu(imagedata, mu_water, mu_bone, hu_bone)\n\n    # Label data\n    if label is not None:\n        labeldata = cls._to_bcdhw(label.data).to(torch.float32)\n    else:\n        labeldata = torch.zeros_like(imagedata)\n\n    isocenter = torch.tensor(image.get_center(), dtype=torch.float32)\n    voxel_to_grid = cls._make_voxel_to_grid(imagedata.shape)\n\n    return cls(\n        imagedata,\n        labeldata,\n        voxel_to_world,\n        world_to_voxel,\n        voxel_to_grid,\n        isocenter,\n        max_label,\n    )\n</code></pre>"},{"location":"reference/nanodrr/data/preprocess/","title":"preprocess","text":""},{"location":"reference/nanodrr/data/preprocess/#nanodrr.data.preprocess","title":"nanodrr.data.preprocess","text":""},{"location":"reference/nanodrr/data/preprocess/#nanodrr.data.preprocess.hu_to_mu","title":"hu_to_mu","text":"<pre><code>hu_to_mu(\n    data: Float[Tensor, \"1 1 D H W\"],\n    mu_water: float = 0.0192,\n    mu_bone: float = 0.0573,\n    hu_bone: float = 1000.0,\n) -&gt; Float[Tensor, \"1 1 D H W\"]\n</code></pre> <p>Convert Hounsfield units to linear attenuation coefficients.</p> <p>Uses bilinear scaling with air-water model for HU &lt;= 0 and water-bone model for HU &gt; 0.</p> PARAMETER DESCRIPTION <code>data</code> <p>CT volume in Hounsfield Units with shape (1, 1, D, H, W).</p> <p> TYPE: <code>Float[Tensor, '1 1 D H W']</code> </p> <code>mu_water</code> <p>Linear attenuation coefficient of water [1/mm] at target energy. Default 0.0192 corresponds to ~70 keV (typical CT effective energy).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0192</code> </p> <code>mu_bone</code> <p>Linear attenuation coefficient of cortical bone [1/mm] at target energy. Default 0.0573 corresponds to ~70 keV.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0573</code> </p> <code>hu_bone</code> <p>HU value corresponding to pure cortical bone. Default 1000. Typical range is 1000-2000 depending on bone type and scanner.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1000.0</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, '1 1 D H W']</code> <p>Linear attenuation coefficients [1/mm] with same shape as input.</p> References <p>NIST XCOM database for mass attenuation coefficients. Water density: 1.0 g/cm\u00b3, cortical bone density: 1.92 g/cm\u00b3.</p> Source code in <code>src/nanodrr/data/preprocess.py</code> <pre><code>def hu_to_mu(\n    data: Float[torch.Tensor, \"1 1 D H W\"],\n    mu_water: float = 0.0192,\n    mu_bone: float = 0.0573,\n    hu_bone: float = 1000.0,\n) -&gt; Float[torch.Tensor, \"1 1 D H W\"]:\n    \"\"\"Convert Hounsfield units to linear attenuation coefficients.\n\n    Uses bilinear scaling with air-water model for HU &lt;= 0 and\n    water-bone model for HU &gt; 0.\n\n    Args:\n        data: CT volume in Hounsfield Units with shape (1, 1, D, H, W).\n        mu_water: Linear attenuation coefficient of water [1/mm] at target\n            energy. Default 0.0192 corresponds to ~70 keV (typical CT\n            effective energy).\n        mu_bone: Linear attenuation coefficient of cortical bone [1/mm] at\n            target energy. Default 0.0573 corresponds to ~70 keV.\n        hu_bone: HU value corresponding to pure cortical bone. Default 1000.\n            Typical range is 1000-2000 depending on bone type and scanner.\n\n    Returns:\n        Linear attenuation coefficients [1/mm] with same shape as input.\n\n    References:\n        NIST XCOM database for mass attenuation coefficients.\n        Water density: 1.0 g/cm\u00b3, cortical bone density: 1.92 g/cm\u00b3.\n    \"\"\"\n    hu_clamped = data.clamp(min=-1000.0)\n\n    mu_low = mu_water * (1.0 + hu_clamped / 1000.0)\n    mu_high = mu_water + (hu_clamped / hu_bone) * (mu_bone - mu_water)\n    mu = torch.where(hu_clamped &lt;= 0, mu_low, mu_high)\n\n    return mu.clamp(min=0.0)\n</code></pre>"},{"location":"reference/nanodrr/drr/","title":"drr","text":""},{"location":"reference/nanodrr/drr/#nanodrr.drr","title":"nanodrr.drr","text":""},{"location":"reference/nanodrr/drr/#nanodrr.drr.DRR","title":"DRR","text":"<pre><code>DRR(\n    k_inv: Float[Tensor, \"B 3 3\"],\n    sdd: Float[Tensor, B],\n    height: int,\n    width: int,\n)\n</code></pre> <p>Digitally reconstructed radiograph (DRR) generator module.</p> <p>Encapsulates the intrinsic camera parameters needed to cast rays from an X-ray point source through a 3D volume. Once initialized, call <code>forward</code> with a <code>Subject</code> and extrinsic pose to produce a synthetic radiograph.</p> <p>The intrinsic parameters (<code>k_inv</code>, <code>sdd</code>, <code>height</code>, <code>width</code>) are stored as buffers or attributes so they travel with the module across devices and are included in <code>state_dict</code>.</p> ATTRIBUTE DESCRIPTION <code>_intrinsic_params</code> <p>Set of parameter names that define the camera intrinsics.</p> <p> </p> PARAMETER DESCRIPTION <code>k_inv</code> <p>Inverse intrinsic camera matrix. Maps pixel coordinates to camera-space ray directions.</p> <p> TYPE: <code>Float[Tensor, 'B 3 3']</code> </p> <code>sdd</code> <p>Source-to-detector distance, i.e., the distance from the X-ray point source to the imaging plane.</p> <p> TYPE: <code>Float[Tensor, B]</code> </p> <code>height</code> <p>Output image height in pixels.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Output image width in pixels.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/nanodrr/drr/drr.py</code> <pre><code>def __init__(\n    self,\n    k_inv: Float[torch.Tensor, \"B 3 3\"],\n    sdd: Float[torch.Tensor, \"B\"],\n    height: int,\n    width: int,\n):\n    super().__init__()\n    self.register_buffer(\"k_inv\", k_inv)\n    self.register_buffer(\"sdd\", sdd)\n    self.height = height\n    self.width = width\n    self._compute_tgt()\n</code></pre>"},{"location":"reference/nanodrr/drr/drr/","title":"drr","text":""},{"location":"reference/nanodrr/drr/drr/#nanodrr.drr.drr","title":"nanodrr.drr.drr","text":""},{"location":"reference/nanodrr/drr/drr/#nanodrr.drr.drr.DRR","title":"DRR","text":"<pre><code>DRR(\n    k_inv: Float[Tensor, \"B 3 3\"],\n    sdd: Float[Tensor, B],\n    height: int,\n    width: int,\n)\n</code></pre> <p>Digitally reconstructed radiograph (DRR) generator module.</p> <p>Encapsulates the intrinsic camera parameters needed to cast rays from an X-ray point source through a 3D volume. Once initialized, call <code>forward</code> with a <code>Subject</code> and extrinsic pose to produce a synthetic radiograph.</p> <p>The intrinsic parameters (<code>k_inv</code>, <code>sdd</code>, <code>height</code>, <code>width</code>) are stored as buffers or attributes so they travel with the module across devices and are included in <code>state_dict</code>.</p> ATTRIBUTE DESCRIPTION <code>_intrinsic_params</code> <p>Set of parameter names that define the camera intrinsics.</p> <p> </p> PARAMETER DESCRIPTION <code>k_inv</code> <p>Inverse intrinsic camera matrix. Maps pixel coordinates to camera-space ray directions.</p> <p> TYPE: <code>Float[Tensor, 'B 3 3']</code> </p> <code>sdd</code> <p>Source-to-detector distance, i.e., the distance from the X-ray point source to the imaging plane.</p> <p> TYPE: <code>Float[Tensor, B]</code> </p> <code>height</code> <p>Output image height in pixels.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Output image width in pixels.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/nanodrr/drr/drr.py</code> <pre><code>def __init__(\n    self,\n    k_inv: Float[torch.Tensor, \"B 3 3\"],\n    sdd: Float[torch.Tensor, \"B\"],\n    height: int,\n    width: int,\n):\n    super().__init__()\n    self.register_buffer(\"k_inv\", k_inv)\n    self.register_buffer(\"sdd\", sdd)\n    self.height = height\n    self.width = width\n    self._compute_tgt()\n</code></pre>"},{"location":"reference/nanodrr/drr/render/","title":"render","text":""},{"location":"reference/nanodrr/drr/render/#nanodrr.drr.render","title":"nanodrr.drr.render","text":""},{"location":"reference/nanodrr/drr/render/#nanodrr.drr.render.render","title":"render","text":"<pre><code>render(\n    subject: Subject,\n    k_inv: Float[Tensor, \"B 3 3\"],\n    rt_inv: Float[Tensor, \"B 4 4\"],\n    sdd: Float[Tensor, B],\n    height: int,\n    width: int,\n    n_samples: int = 500,\n    align_corners: bool = True,\n    src: Float[Tensor, \"B (H W) 3\"] | None = None,\n    tgt: Float[Tensor, \"B (H W) 3\"] | None = None,\n) -&gt; Float[Tensor, \"B C H W\"]\n</code></pre> <p>Differentiable ray marching through a volume and optional labelmap.</p> <p>Casts rays from an X-ray source through a 3D volume (<code>Subject.image</code>) and integrates sampled intensities along each ray to produce a synthetic radiograph. When the subject contains a multi-class labelmap (<code>Subject.label</code>), the integration is performed per-structure, yielding one channel per class.</p> PARAMETER DESCRIPTION <code>subject</code> <p>The volume to render. Must contain <code>Subject.image</code> (the 3D density volume) and optionally <code>Subject.label</code> (a multi-class labelmap for per-structure integration).</p> <p> TYPE: <code>Subject</code> </p> <code>k_inv</code> <p>Inverse intrinsic camera matrix. Maps pixel coordinates to camera-space ray directions.</p> <p> TYPE: <code>Float[Tensor, 'B 3 3']</code> </p> <code>rt_inv</code> <p>Inverse extrinsic (world-to-camera) matrix. Transforms rays from camera space into world space.</p> <p> TYPE: <code>Float[Tensor, 'B 4 4']</code> </p> <code>sdd</code> <p>Source-to-detector distance, i.e., the distance from the X-ray point source to the imaging plane.</p> <p> TYPE: <code>Float[Tensor, B]</code> </p> <code>height</code> <p>Output image height in pixels.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Output image width in pixels.</p> <p> TYPE: <code>int</code> </p> <code>n_samples</code> <p>Number of samples to take along each ray. Higher values improve accuracy at the cost of memory and compute.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>align_corners</code> <p>If <code>True</code>, the voxel grid corners are aligned with the volume boundaries (consistent with <code>torch.nn.functional.grid_sample</code>).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>src</code> <p>Pre-computed ray source positions in world coordinates. If <code>None</code>, computed from <code>k_inv</code> and <code>rt_inv</code>.</p> <p> TYPE: <code>Float[Tensor, 'B (H W) 3'] | None</code> DEFAULT: <code>None</code> </p> <code>tgt</code> <p>Pre-computed ray target positions (detector pixel locations) in world coordinates. If <code>None</code>, computed from <code>k_inv</code> and <code>rt_inv</code>.</p> <p> TYPE: <code>Float[Tensor, 'B (H W) 3'] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, 'B C H W']</code> <p>Rendered synthetic radiograph. Shape is <code>(B, C, H, W)</code> where <code>C</code> is the number of classes in the labelmap (or 1 if no labelmap is present).</p> Source code in <code>src/nanodrr/drr/render.py</code> <pre><code>def render(\n    subject: Subject,\n    k_inv: Float[torch.Tensor, \"B 3 3\"],\n    rt_inv: Float[torch.Tensor, \"B 4 4\"],\n    sdd: Float[torch.Tensor, \"B\"],\n    height: int,\n    width: int,\n    n_samples: int = 500,\n    align_corners: bool = True,\n    src: Float[torch.Tensor, \"B (H W) 3\"] | None = None,\n    tgt: Float[torch.Tensor, \"B (H W) 3\"] | None = None,\n) -&gt; Float[torch.Tensor, \"B C H W\"]:\n    \"\"\"Differentiable ray marching through a volume and optional labelmap.\n\n    Casts rays from an X-ray source through a 3D volume (`Subject.image`) and\n    integrates sampled intensities along each ray to produce a synthetic\n    radiograph. When the subject contains a multi-class labelmap (`Subject.label`),\n    the integration is performed per-structure, yielding one channel per class.\n\n    Args:\n        subject: The volume to render. Must contain `Subject.image` (the 3D\n            density volume) and optionally `Subject.label` (a multi-class\n            labelmap for per-structure integration).\n        k_inv: Inverse intrinsic camera matrix. Maps pixel coordinates to\n            camera-space ray directions.\n        rt_inv: Inverse extrinsic (world-to-camera) matrix. Transforms rays\n            from camera space into world space.\n        sdd: Source-to-detector distance, i.e., the distance from the X-ray\n            point source to the imaging plane.\n        height: Output image height in pixels.\n        width: Output image width in pixels.\n        n_samples: Number of samples to take along each ray. Higher values\n            improve accuracy at the cost of memory and compute.\n        align_corners: If `True`, the voxel grid corners are aligned with the\n            volume boundaries (consistent with `torch.nn.functional.grid_sample`).\n        src: Pre-computed ray source positions in world coordinates. If `None`,\n            computed from `k_inv` and `rt_inv`.\n        tgt: Pre-computed ray target positions (detector pixel locations) in\n            world coordinates. If `None`, computed from `k_inv` and `rt_inv`.\n\n    Returns:\n        Rendered synthetic radiograph. Shape is `(B, C, H, W)` where `C` is\n            the number of classes in the labelmap (or 1 if no labelmap is\n            present).\n    \"\"\"\n    device, dtype = rt_inv.device, rt_inv.dtype\n    B = rt_inv.shape[0]\n    C = subject.n_classes\n    N = height * width\n\n    # Get the ray endpoints in camera coordinates\n    if src is None:\n        src = torch.zeros(B, 1, 3, device=device, dtype=dtype)\n    if tgt is None:\n        tgt = _make_tgt(k_inv, sdd, height, width, device, dtype)\n\n    # Compute step size [mm] in camera space\n    step_size = (tgt - src).norm(dim=-1) / float(n_samples - 1)\n\n    # Change coordinates: camera \u2192 world \u2192 voxel \u2192 normalized grid\n    xform = subject.world_to_grid @ rt_inv\n    src = transform_point(xform, src)\n    tgt = transform_point(xform, tgt)\n\n    # Linearly interpolate sample points along each ray\n    t = torch.linspace(0, 1, n_samples, device=device, dtype=src.dtype)\n    pts = torch.lerp(\n        src[:, None, :, None],\n        tgt[:, None, :, None],\n        t[None, :, None, None, None],\n    )\n\n    # Sample the volume\n    img = F.grid_sample(\n        subject.image.expand(B, -1, -1, -1, -1),\n        pts,\n        mode=\"bilinear\",\n        align_corners=align_corners,\n    )[:, 0, ..., 0]  # [B, n_samples, N]\n    img = img * step_size[:, None, :]\n\n    if C == 1:  # Compute whole-volume ray marching\n        return img.sum(dim=1, keepdim=True).reshape(B, C, height, width)\n\n    # Sample the mask\n    idx = F.grid_sample(\n        subject.label.expand(B, -1, -1, -1, -1),\n        pts,\n        mode=\"nearest\",\n        align_corners=align_corners,\n    )[:, 0, ..., 0].long()  # [B, n_samples, N]\n\n    # Compute the structure-specific ray marching\n    out = torch.zeros(B, C, N, device=img.device, dtype=img.dtype)\n    out.scatter_add_(1, idx, img)\n    return out.reshape(B, C, height, width)\n</code></pre>"},{"location":"reference/nanodrr/geometry/","title":"geometry","text":""},{"location":"reference/nanodrr/geometry/#nanodrr.geometry","title":"nanodrr.geometry","text":""},{"location":"reference/nanodrr/geometry/#nanodrr.geometry.transform_point","title":"transform_point","text":"<pre><code>transform_point(\n    xform: Float[Tensor, \"B *_ N+1 N+1\"], v: Float[Tensor, \"B *_ N\"]\n) -&gt; Float[Tensor, \"B *_ N\"]\n</code></pre> <p>Apply homogeneous transformation to points.</p> PARAMETER DESCRIPTION <code>xform</code> <p>Transformation matrix of shape (B, ..., N+1, N+1).</p> <p> TYPE: <code>Float[Tensor, 'B *_ N+1 N+1']</code> </p> <code>v</code> <p>Points of shape (B, ..., N).</p> <p> TYPE: <code>Float[Tensor, 'B *_ N']</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, 'B *_ N']</code> <p>Transformed points of shape (B, ..., N).</p> Source code in <code>src/nanodrr/geometry/transform.py</code> <pre><code>def transform_point(\n    xform: Float[torch.Tensor, \"B *_ N+1 N+1\"],\n    v: Float[torch.Tensor, \"B *_ N\"],\n) -&gt; Float[torch.Tensor, \"B *_ N\"]:\n    \"\"\"Apply homogeneous transformation to points.\n\n    Args:\n        xform: Transformation matrix of shape (B, ..., N+1, N+1).\n        v: Points of shape (B, ..., N).\n\n    Returns:\n        Transformed points of shape (B, ..., N).\n    \"\"\"\n    return _dehomo(torch.einsum(\"b...ij,b...j-&gt;b...i\", xform, _homo(v)))\n</code></pre>"},{"location":"reference/nanodrr/geometry/#nanodrr.geometry.convert","title":"convert","text":"<pre><code>convert(\n    rotation: Float[Tensor, \"B D\"],\n    translation: Float[Tensor, \"B 3\"],\n    parameterization: Parameterization,\n    convention: str = None,\n    degrees: bool = True,\n    isocenter: Float[Tensor, 3] | None = None,\n) -&gt; Float[Tensor, \"B 4 4\"]\n</code></pre> <p>Convert a rotation parameterization + camera center into a (B, 4, 4) SE(3) matrix.</p> <p>The translation is interpreted as the camera center in world coordinates, i.e. the resulting matrix stores t = R @ translation.</p> PARAMETER DESCRIPTION <code>rotation</code> <p>Rotation parameters, shape depends on parameterization:</p> <ul> <li><code>EULER</code>:                (B, 3) Euler angles</li> <li><code>QUATERNION</code>:           (B, 4) unit quaternion (XYZW)</li> <li><code>QUATERNION_ADJUGATE</code>:  (B, 10) upper-tri of 4x4 symmetric matrix</li> <li><code>ROTATION_9D</code>:          (B, 9) flattened 3x3 matrix (projected via SVD)</li> <li><code>SE3_LOG</code>:              (B, 3) rotation part of se(3) logarithm</li> </ul> <p> TYPE: <code>Float[Tensor, 'B D']</code> </p> <code>translation</code> <p>Camera center in world coordinates, shape (B, 3). For <code>SE3_LOG</code> this is the log-translation (coupled via the V-matrix).</p> <p> TYPE: <code>Float[Tensor, 'B 3']</code> </p> <code>parameterization</code> <p>Which rotation representation to use.</p> <p> TYPE: <code>Parameterization</code> </p> <code>convention</code> <p>Required for EULER only. 3-letter string from {X, Y, Z}, e.g., \"XYZ\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>degrees</code> <p>If <code>True</code> and parameterization is EULER, interpret angles in degrees.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>isocenter</code> <p>If provided, the rotation is applied about this point instead of the world origin. The resulting translation is adjusted so that the camera orbits around <code>isocenter</code>.</p> <p> TYPE: <code>Float[Tensor, 3] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, 'B 4 4']</code> <p>Batched SE(3) matrices of shape (B, 4, 4).</p> Source code in <code>src/nanodrr/geometry/se3.py</code> <pre><code>def convert(\n    rotation: Float[torch.Tensor, \"B D\"],\n    translation: Float[torch.Tensor, \"B 3\"],\n    parameterization: Parameterization,\n    convention: str = None,\n    degrees: bool = True,\n    isocenter: Float[torch.Tensor, \"3\"] | None = None,\n) -&gt; Float[torch.Tensor, \"B 4 4\"]:\n    \"\"\"Convert a rotation parameterization + camera center into a (B, 4, 4) SE(3) matrix.\n\n    The translation is interpreted as the camera center in world coordinates,\n    i.e. the resulting matrix stores t = R @ translation.\n\n    Args:\n        rotation: Rotation parameters, shape depends on parameterization:\n\n            - `EULER`:                (B, 3) Euler angles\n            - `QUATERNION`:           (B, 4) unit quaternion (XYZW)\n            - `QUATERNION_ADJUGATE`:  (B, 10) upper-tri of 4x4 symmetric matrix\n            - `ROTATION_9D`:          (B, 9) flattened 3x3 matrix (projected via SVD)\n            - `SE3_LOG`:              (B, 3) rotation part of se(3) logarithm\n        translation: Camera center in world coordinates, shape (B, 3).\n            For `SE3_LOG` this is the log-translation (coupled via the V-matrix).\n        parameterization: Which rotation representation to use.\n        convention: Required for EULER only. 3-letter string from {X, Y, Z}, e.g., \"XYZ\".\n        degrees: If `True` and parameterization is EULER, interpret angles in degrees.\n        isocenter: If provided, the rotation is applied about this point\n            instead of the world origin. The resulting translation is adjusted\n            so that the camera orbits around `isocenter`.\n\n    Returns:\n        Batched SE(3) matrices of shape (B, 4, 4).\n    \"\"\"\n    parameterization = Parameterization(parameterization)\n\n    if parameterization == Parameterization.SE3_LOG:\n        return _se3_exp_map(rotation, translation)\n\n    R = rotation_to_matrix(rotation, parameterization, convention, degrees)\n    t = torch.einsum(\"bij, bj -&gt; bi\", R, translation)\n    if isocenter is not None:\n        t = t + isocenter\n    return make_se3(R, t)\n</code></pre>"},{"location":"reference/nanodrr/geometry/se3/","title":"se3","text":""},{"location":"reference/nanodrr/geometry/se3/#nanodrr.geometry.se3","title":"nanodrr.geometry.se3","text":""},{"location":"reference/nanodrr/geometry/se3/#nanodrr.geometry.se3.convert","title":"convert","text":"<pre><code>convert(\n    rotation: Float[Tensor, \"B D\"],\n    translation: Float[Tensor, \"B 3\"],\n    parameterization: Parameterization,\n    convention: str = None,\n    degrees: bool = True,\n    isocenter: Float[Tensor, 3] | None = None,\n) -&gt; Float[Tensor, \"B 4 4\"]\n</code></pre> <p>Convert a rotation parameterization + camera center into a (B, 4, 4) SE(3) matrix.</p> <p>The translation is interpreted as the camera center in world coordinates, i.e. the resulting matrix stores t = R @ translation.</p> PARAMETER DESCRIPTION <code>rotation</code> <p>Rotation parameters, shape depends on parameterization:</p> <ul> <li><code>EULER</code>:                (B, 3) Euler angles</li> <li><code>QUATERNION</code>:           (B, 4) unit quaternion (XYZW)</li> <li><code>QUATERNION_ADJUGATE</code>:  (B, 10) upper-tri of 4x4 symmetric matrix</li> <li><code>ROTATION_9D</code>:          (B, 9) flattened 3x3 matrix (projected via SVD)</li> <li><code>SE3_LOG</code>:              (B, 3) rotation part of se(3) logarithm</li> </ul> <p> TYPE: <code>Float[Tensor, 'B D']</code> </p> <code>translation</code> <p>Camera center in world coordinates, shape (B, 3). For <code>SE3_LOG</code> this is the log-translation (coupled via the V-matrix).</p> <p> TYPE: <code>Float[Tensor, 'B 3']</code> </p> <code>parameterization</code> <p>Which rotation representation to use.</p> <p> TYPE: <code>Parameterization</code> </p> <code>convention</code> <p>Required for EULER only. 3-letter string from {X, Y, Z}, e.g., \"XYZ\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>degrees</code> <p>If <code>True</code> and parameterization is EULER, interpret angles in degrees.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>isocenter</code> <p>If provided, the rotation is applied about this point instead of the world origin. The resulting translation is adjusted so that the camera orbits around <code>isocenter</code>.</p> <p> TYPE: <code>Float[Tensor, 3] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, 'B 4 4']</code> <p>Batched SE(3) matrices of shape (B, 4, 4).</p> Source code in <code>src/nanodrr/geometry/se3.py</code> <pre><code>def convert(\n    rotation: Float[torch.Tensor, \"B D\"],\n    translation: Float[torch.Tensor, \"B 3\"],\n    parameterization: Parameterization,\n    convention: str = None,\n    degrees: bool = True,\n    isocenter: Float[torch.Tensor, \"3\"] | None = None,\n) -&gt; Float[torch.Tensor, \"B 4 4\"]:\n    \"\"\"Convert a rotation parameterization + camera center into a (B, 4, 4) SE(3) matrix.\n\n    The translation is interpreted as the camera center in world coordinates,\n    i.e. the resulting matrix stores t = R @ translation.\n\n    Args:\n        rotation: Rotation parameters, shape depends on parameterization:\n\n            - `EULER`:                (B, 3) Euler angles\n            - `QUATERNION`:           (B, 4) unit quaternion (XYZW)\n            - `QUATERNION_ADJUGATE`:  (B, 10) upper-tri of 4x4 symmetric matrix\n            - `ROTATION_9D`:          (B, 9) flattened 3x3 matrix (projected via SVD)\n            - `SE3_LOG`:              (B, 3) rotation part of se(3) logarithm\n        translation: Camera center in world coordinates, shape (B, 3).\n            For `SE3_LOG` this is the log-translation (coupled via the V-matrix).\n        parameterization: Which rotation representation to use.\n        convention: Required for EULER only. 3-letter string from {X, Y, Z}, e.g., \"XYZ\".\n        degrees: If `True` and parameterization is EULER, interpret angles in degrees.\n        isocenter: If provided, the rotation is applied about this point\n            instead of the world origin. The resulting translation is adjusted\n            so that the camera orbits around `isocenter`.\n\n    Returns:\n        Batched SE(3) matrices of shape (B, 4, 4).\n    \"\"\"\n    parameterization = Parameterization(parameterization)\n\n    if parameterization == Parameterization.SE3_LOG:\n        return _se3_exp_map(rotation, translation)\n\n    R = rotation_to_matrix(rotation, parameterization, convention, degrees)\n    t = torch.einsum(\"bij, bj -&gt; bi\", R, translation)\n    if isocenter is not None:\n        t = t + isocenter\n    return make_se3(R, t)\n</code></pre>"},{"location":"reference/nanodrr/geometry/se3/#nanodrr.geometry.se3.rotation_to_matrix","title":"rotation_to_matrix","text":"<pre><code>rotation_to_matrix(\n    rotation: Float[Tensor, \"B D\"],\n    parameterization: Parameterization,\n    convention: str = None,\n    degrees: bool = False,\n) -&gt; Float[Tensor, \"B 3 3\"]\n</code></pre> <p>Convert rotation parameters into a (B, 3, 3) rotation matrix.</p> PARAMETER DESCRIPTION <code>rotation</code> <p>Rotation parameters (shape depends on parameterization).</p> <p> TYPE: <code>Float[Tensor, 'B D']</code> </p> <code>parameterization</code> <p>Which rotation representation to use.</p> <p> TYPE: <code>Parameterization</code> </p> <code>convention</code> <p>Required for EULER. 3-letter string from {X, Y, Z}.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>degrees</code> <p>If True and EULER, interpret angles in degrees.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, 'B 3 3']</code> <p>Batched rotation matrices of shape (B, 3, 3).</p> Source code in <code>src/nanodrr/geometry/se3.py</code> <pre><code>def rotation_to_matrix(\n    rotation: Float[torch.Tensor, \"B D\"],\n    parameterization: Parameterization,\n    convention: str = None,\n    degrees: bool = False,\n) -&gt; Float[torch.Tensor, \"B 3 3\"]:\n    \"\"\"Convert rotation parameters into a (B, 3, 3) rotation matrix.\n\n    Args:\n        rotation: Rotation parameters (shape depends on parameterization).\n        parameterization: Which rotation representation to use.\n        convention: Required for EULER. 3-letter string from {X, Y, Z}.\n        degrees: If True and EULER, interpret angles in degrees.\n\n    Returns:\n        Batched rotation matrices of shape (B, 3, 3).\n    \"\"\"\n    parameterization = Parameterization(parameterization)\n\n    if parameterization == Parameterization.EULER:\n        if convention is None:\n            raise ValueError(\"convention must be specified for Euler angles (e.g., 'XYZ', 'ZYX')\")\n        if degrees:\n            rotation = rotation * (torch.pi / 180.0)\n        return roma.euler_to_rotmat(convention, rotation)\n\n    if parameterization == Parameterization.QUATERNION:\n        return roma.unitquat_to_rotmat(rotation)\n\n    if parameterization == Parameterization.QUATERNION_ADJUGATE:\n        q = quaternion_adjugate_to_quaternion(rotation)\n        q = roma.quat_wxyz_to_xyzw(q)\n        return roma.unitquat_to_rotmat(q)\n\n    if parameterization == Parameterization.ROTATION_9D:\n        return roma.special_procrustes(rotation.reshape(-1, 3, 3))\n\n    if parameterization == Parameterization.SO3_LOG:\n        return roma.rotvec_to_rotmat(rotation)\n\n    if parameterization == Parameterization.SE3_LOG:\n        return roma.rotvec_to_rotmat(rotation)\n\n    raise ValueError(f\"Unknown parameterization: {parameterization}\")\n</code></pre>"},{"location":"reference/nanodrr/geometry/se3/#nanodrr.geometry.se3.make_se3","title":"make_se3","text":"<pre><code>make_se3(\n    R: Float[Tensor, \"B 3 3\"], t: Float[Tensor, \"B 3\"]\n) -&gt; Float[Tensor, \"B 4 4\"]\n</code></pre> <p>Assemble a (B, 4, 4) SE(3) matrix from rotation and translation.</p> PARAMETER DESCRIPTION <code>R</code> <p>Rotation matrices of shape (B, 3, 3).</p> <p> TYPE: <code>Float[Tensor, 'B 3 3']</code> </p> <code>t</code> <p>Translation vectors of shape (B, 3).</p> <p> TYPE: <code>Float[Tensor, 'B 3']</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, 'B 4 4']</code> <p>Homogeneous transformation matrices of shape (B, 4, 4).</p> Source code in <code>src/nanodrr/geometry/se3.py</code> <pre><code>def make_se3(R: Float[torch.Tensor, \"B 3 3\"], t: Float[torch.Tensor, \"B 3\"]) -&gt; Float[torch.Tensor, \"B 4 4\"]:\n    \"\"\"Assemble a (B, 4, 4) SE(3) matrix from rotation and translation.\n\n    Args:\n        R: Rotation matrices of shape (B, 3, 3).\n        t: Translation vectors of shape (B, 3).\n\n    Returns:\n        Homogeneous transformation matrices of shape (B, 4, 4).\n    \"\"\"\n    B = R.shape[0]\n    T = torch.zeros(B, 4, 4, dtype=R.dtype, device=R.device)\n    T[:, :3, :3] = R\n    T[:, :3, 3] = t\n    T[:, 3, 3] = 1.0\n    return T\n</code></pre>"},{"location":"reference/nanodrr/geometry/se3/#nanodrr.geometry.se3.quaternion_adjugate_to_quaternion","title":"quaternion_adjugate_to_quaternion","text":"<pre><code>quaternion_adjugate_to_quaternion(\n    rotation: Float[Tensor, \"B 10\"],\n) -&gt; Float[Tensor, \"B 4\"]\n</code></pre> <p>Convert a 10D quaternion-adjugate vector to a unit quaternion.</p> <p>The 10D vector encodes the upper triangle of a symmetric 4x4 matrix (the quaternion adjugate). The quaternion is recovered as the column with largest norm, avoiding an explicit eigendecomposition via a fast method.</p> <p>Reference: https://arxiv.org/abs/2205.09116</p> PARAMETER DESCRIPTION <code>rotation</code> <p>(B, 10) upper-triangular entries of the 4x4 symmetric matrix.</p> <p> TYPE: <code>Float[Tensor, 'B 10']</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, 'B 4']</code> <p>Unit quaternions of shape (B, 4).</p> Source code in <code>src/nanodrr/geometry/se3.py</code> <pre><code>def quaternion_adjugate_to_quaternion(rotation: Float[torch.Tensor, \"B 10\"]) -&gt; Float[torch.Tensor, \"B 4\"]:\n    \"\"\"Convert a 10D quaternion-adjugate vector to a unit quaternion.\n\n    The 10D vector encodes the upper triangle of a symmetric 4x4 matrix (the\n    quaternion adjugate). The quaternion is recovered as the column with largest\n    norm, avoiding an explicit eigendecomposition via a fast method.\n\n    Reference: https://arxiv.org/abs/2205.09116\n\n    Args:\n        rotation: (B, 10) upper-triangular entries of the 4x4 symmetric matrix.\n\n    Returns:\n        Unit quaternions of shape (B, 4).\n    \"\"\"\n    A = _vec10_to_symmetric4x4(rotation)\n    norms = A.norm(dim=1).amax(dim=1, keepdim=True)\n    max_eigenvectors = torch.argmax(A.norm(dim=1), dim=1)\n    return A[range(len(A)), max_eigenvectors] / norms\n</code></pre>"},{"location":"reference/nanodrr/geometry/se3/#nanodrr.geometry.se3.se3_log_map","title":"se3_log_map","text":"<pre><code>se3_log_map(\n    matrix: Float[Tensor, \"B 4 4\"],\n) -&gt; tuple[Float[Tensor, \"B 3\"], Float[Tensor, \"B 3\"]]\n</code></pre> <p>Compute the SE(3) logarithm of a batch of 4x4 transformation matrices.</p> PARAMETER DESCRIPTION <code>matrix</code> <p>(B, 4, 4) SE(3) matrices.</p> <p> TYPE: <code>Float[Tensor, 'B 4 4']</code> </p> RETURNS DESCRIPTION <code>tuple[Float[Tensor, 'B 3'], Float[Tensor, 'B 3']]</code> <p>Tuple of (log_rotation, log_translation), each (B, 3).</p> Source code in <code>src/nanodrr/geometry/se3.py</code> <pre><code>def se3_log_map(matrix: Float[torch.Tensor, \"B 4 4\"]) -&gt; tuple[Float[torch.Tensor, \"B 3\"], Float[torch.Tensor, \"B 3\"]]:\n    \"\"\"Compute the SE(3) logarithm of a batch of 4x4 transformation matrices.\n\n    Args:\n        matrix: (B, 4, 4) SE(3) matrices.\n\n    Returns:\n        Tuple of (log_rotation, log_translation), each (B, 3).\n    \"\"\"\n    R = matrix[:, :3, :3]\n    t = matrix[:, :3, 3]\n\n    log_rotation = roma.rotmat_to_rotvec(R)\n    V = _se3_V_matrix(log_rotation)\n    log_translation = torch.linalg.solve(V, t.unsqueeze(-1)).squeeze(-1)\n\n    return log_rotation, log_translation\n</code></pre>"},{"location":"reference/nanodrr/geometry/transform/","title":"transform","text":""},{"location":"reference/nanodrr/geometry/transform/#nanodrr.geometry.transform","title":"nanodrr.geometry.transform","text":""},{"location":"reference/nanodrr/geometry/transform/#nanodrr.geometry.transform.transform_point","title":"transform_point","text":"<pre><code>transform_point(\n    xform: Float[Tensor, \"B *_ N+1 N+1\"], v: Float[Tensor, \"B *_ N\"]\n) -&gt; Float[Tensor, \"B *_ N\"]\n</code></pre> <p>Apply homogeneous transformation to points.</p> PARAMETER DESCRIPTION <code>xform</code> <p>Transformation matrix of shape (B, ..., N+1, N+1).</p> <p> TYPE: <code>Float[Tensor, 'B *_ N+1 N+1']</code> </p> <code>v</code> <p>Points of shape (B, ..., N).</p> <p> TYPE: <code>Float[Tensor, 'B *_ N']</code> </p> RETURNS DESCRIPTION <code>Float[Tensor, 'B *_ N']</code> <p>Transformed points of shape (B, ..., N).</p> Source code in <code>src/nanodrr/geometry/transform.py</code> <pre><code>def transform_point(\n    xform: Float[torch.Tensor, \"B *_ N+1 N+1\"],\n    v: Float[torch.Tensor, \"B *_ N\"],\n) -&gt; Float[torch.Tensor, \"B *_ N\"]:\n    \"\"\"Apply homogeneous transformation to points.\n\n    Args:\n        xform: Transformation matrix of shape (B, ..., N+1, N+1).\n        v: Points of shape (B, ..., N).\n\n    Returns:\n        Transformed points of shape (B, ..., N).\n    \"\"\"\n    return _dehomo(torch.einsum(\"b...ij,b...j-&gt;b...i\", xform, _homo(v)))\n</code></pre>"},{"location":"reference/nanodrr/metrics/","title":"metrics","text":""},{"location":"reference/nanodrr/metrics/#nanodrr.metrics","title":"nanodrr.metrics","text":""},{"location":"reference/nanodrr/metrics/#nanodrr.metrics.DoubleGeodesicSE3","title":"DoubleGeodesicSE3","text":"<pre><code>DoubleGeodesicSE3(sdd: float, eps: float = 1e-07)\n</code></pre> <p>Calculate the angular and translational geodesics between two SE(3) transformation matrices.</p> Source code in <code>src/nanodrr/metrics/geo.py</code> <pre><code>def __init__(\n    self,\n    sdd: float,\n    eps: float = 1e-7,\n):\n    super().__init__()\n    self.sdr = sdd / 2\n    self.eps = eps\n</code></pre>"},{"location":"reference/nanodrr/metrics/#nanodrr.metrics.GradientNormalizedCrossCorrelation2d","title":"GradientNormalizedCrossCorrelation2d","text":"<pre><code>GradientNormalizedCrossCorrelation2d(\n    patch_size: int | None = None, sigma: float = 1.0, **kwargs\n)\n</code></pre> <p>Compute Normalized Cross Correlation between the image gradients of two batches of images.</p> Source code in <code>src/nanodrr/metrics/ncc.py</code> <pre><code>def __init__(\n    self,\n    patch_size: int | None = None,\n    sigma: float = 1.0,\n    **kwargs,\n):\n    super().__init__(patch_size, **kwargs)\n    self.sobel = _Sobel(sigma)\n</code></pre>"},{"location":"reference/nanodrr/metrics/#nanodrr.metrics.MultiscaleNormalizedCrossCorrelation2d","title":"MultiscaleNormalizedCrossCorrelation2d","text":"<pre><code>MultiscaleNormalizedCrossCorrelation2d(\n    patch_sizes: list[int | None] = [None],\n    patch_weights: list[float] = [1.0],\n    eps: float = 0.0001,\n)\n</code></pre> <p>Compute Normalized Cross Correlation between two batches of images at multiple scales.</p> Source code in <code>src/nanodrr/metrics/ncc.py</code> <pre><code>def __init__(\n    self,\n    patch_sizes: list[int | None] = [None],\n    patch_weights: list[float] = [1.0],\n    eps: float = 1e-4,\n):\n    super().__init__()\n    assert len(patch_sizes) == len(patch_weights), \"Each scale must have a weight\"\n    self.nccs = torch.nn.ModuleList([NormalizedCrossCorrelation2d(patch_size, eps) for patch_size in patch_sizes])\n    self.patch_weights = patch_weights\n</code></pre>"},{"location":"reference/nanodrr/metrics/#nanodrr.metrics.NormalizedCrossCorrelation2d","title":"NormalizedCrossCorrelation2d","text":"<pre><code>NormalizedCrossCorrelation2d(patch_size: int | None = None, eps: float = 1e-07)\n</code></pre> <p>Compute Normalized Cross Correlation between two batches of images.</p> Source code in <code>src/nanodrr/metrics/ncc.py</code> <pre><code>def __init__(self, patch_size: int | None = None, eps: float = 1e-7):\n    super().__init__()\n    self.patch_size = patch_size\n    self.eps = eps\n</code></pre>"},{"location":"reference/nanodrr/metrics/geo/","title":"geo","text":""},{"location":"reference/nanodrr/metrics/geo/#nanodrr.metrics.geo","title":"nanodrr.metrics.geo","text":""},{"location":"reference/nanodrr/metrics/geo/#nanodrr.metrics.geo.DoubleGeodesicSE3","title":"DoubleGeodesicSE3","text":"<pre><code>DoubleGeodesicSE3(sdd: float, eps: float = 1e-07)\n</code></pre> <p>Calculate the angular and translational geodesics between two SE(3) transformation matrices.</p> Source code in <code>src/nanodrr/metrics/geo.py</code> <pre><code>def __init__(\n    self,\n    sdd: float,\n    eps: float = 1e-7,\n):\n    super().__init__()\n    self.sdr = sdd / 2\n    self.eps = eps\n</code></pre>"},{"location":"reference/nanodrr/metrics/ncc/","title":"ncc","text":""},{"location":"reference/nanodrr/metrics/ncc/#nanodrr.metrics.ncc","title":"nanodrr.metrics.ncc","text":""},{"location":"reference/nanodrr/metrics/ncc/#nanodrr.metrics.ncc.NormalizedCrossCorrelation2d","title":"NormalizedCrossCorrelation2d","text":"<pre><code>NormalizedCrossCorrelation2d(patch_size: int | None = None, eps: float = 1e-07)\n</code></pre> <p>Compute Normalized Cross Correlation between two batches of images.</p> Source code in <code>src/nanodrr/metrics/ncc.py</code> <pre><code>def __init__(self, patch_size: int | None = None, eps: float = 1e-7):\n    super().__init__()\n    self.patch_size = patch_size\n    self.eps = eps\n</code></pre>"},{"location":"reference/nanodrr/metrics/ncc/#nanodrr.metrics.ncc.MultiscaleNormalizedCrossCorrelation2d","title":"MultiscaleNormalizedCrossCorrelation2d","text":"<pre><code>MultiscaleNormalizedCrossCorrelation2d(\n    patch_sizes: list[int | None] = [None],\n    patch_weights: list[float] = [1.0],\n    eps: float = 0.0001,\n)\n</code></pre> <p>Compute Normalized Cross Correlation between two batches of images at multiple scales.</p> Source code in <code>src/nanodrr/metrics/ncc.py</code> <pre><code>def __init__(\n    self,\n    patch_sizes: list[int | None] = [None],\n    patch_weights: list[float] = [1.0],\n    eps: float = 1e-4,\n):\n    super().__init__()\n    assert len(patch_sizes) == len(patch_weights), \"Each scale must have a weight\"\n    self.nccs = torch.nn.ModuleList([NormalizedCrossCorrelation2d(patch_size, eps) for patch_size in patch_sizes])\n    self.patch_weights = patch_weights\n</code></pre>"},{"location":"reference/nanodrr/metrics/ncc/#nanodrr.metrics.ncc.GradientNormalizedCrossCorrelation2d","title":"GradientNormalizedCrossCorrelation2d","text":"<pre><code>GradientNormalizedCrossCorrelation2d(\n    patch_size: int | None = None, sigma: float = 1.0, **kwargs\n)\n</code></pre> <p>Compute Normalized Cross Correlation between the image gradients of two batches of images.</p> Source code in <code>src/nanodrr/metrics/ncc.py</code> <pre><code>def __init__(\n    self,\n    patch_size: int | None = None,\n    sigma: float = 1.0,\n    **kwargs,\n):\n    super().__init__(patch_size, **kwargs)\n    self.sobel = _Sobel(sigma)\n</code></pre>"},{"location":"reference/nanodrr/plot/","title":"plot","text":""},{"location":"reference/nanodrr/plot/#nanodrr.plot","title":"nanodrr.plot","text":""},{"location":"reference/nanodrr/plot/#nanodrr.plot.plot_drr","title":"plot_drr","text":"<pre><code>plot_drr(\n    img: Float[Tensor, \"B C H W\"],\n    mask: Bool[Tensor, \"B C H W\"] | None = None,\n    title: list[str] | None = None,\n    ticks: bool = True,\n    axs: list[Axes] | None = None,\n    cmap: str = \"gray\",\n    mask_cmap: str | Colormap = \"Set2\",\n    mask_n_colors: int = 7,\n    interior_alpha: float = 0.3,\n    edge_alpha: float = 1.0,\n    edge_width: int = 1,\n    **imshow_kwargs\n) -&gt; list[Axes]\n</code></pre> <p>Plot a batch of DRR images, optionally with a segmentation mask overlay.</p> <p>Renders each image by summing across channels, simulating X-ray intensity accumulation along a ray. A segmentation mask can be overlaid in two ways: passed explicitly via <code>mask</code>, or derived automatically when <code>img</code> has more than one channel (where channel 0 is background and channels 1+ are labeled structures). These two modes are mutually exclusive.</p> <p>When a mask is rendered, channel 0 is always dropped. It is assumed to represent background. Each remaining channel is drawn with a distinct color, a translucent interior fill, and an opaque boundary edge detected via morphological erosion.</p> PARAMETER DESCRIPTION <code>img</code> <p>Batch of DRR images with shape <code>(B, C, H, W)</code>. If <code>C &gt; 1</code>, channels 1+ are treated as binary segmentation labels and a mask is derived as <code>img &gt; 0</code>. Channel intensities are summed across <code>C</code> for display.</p> <p> TYPE: <code>Float[Tensor, 'B C H W']</code> </p> <code>mask</code> <p>Explicit segmentation mask with shape <code>(B, C, H, W)</code>, where channel 0 is background and channels 1+ are labeled structures. Mutually exclusive with a multi-channel <code>img</code>.</p> <p> TYPE: <code>Bool[Tensor, 'B C H W'] | None</code> DEFAULT: <code>None</code> </p> <code>title</code> <p>Per-image labels of length <code>B</code>, rendered as x-axis titles. If <code>None</code>, no labels are shown.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>ticks</code> <p>Whether to display 1-indexed pixel coordinate ticks. If <code>False</code>, all tick marks are hidden. Defaults to <code>True</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>axs</code> <p>Pre-existing axes to plot into. Must have length <code>B</code>. If <code>None</code>, a new figure with <code>B</code> subplots is created.</p> <p> TYPE: <code>list[Axes] | None</code> DEFAULT: <code>None</code> </p> <code>cmap</code> <p>Colormap for the DRR image. Defaults to <code>\"gray\"</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'gray'</code> </p> <code>mask_cmap</code> <p>Colormap used to assign colors to segmentation channels. Colors are sampled evenly and cycled if the number of channels exceeds <code>mask_n_colors</code>. Defaults to <code>\"Set2\"</code>.</p> <p> TYPE: <code>str | Colormap</code> DEFAULT: <code>'Set2'</code> </p> <code>mask_n_colors</code> <p>Number of evenly spaced colors to sample from <code>mask_cmap</code> before cycling. Defaults to <code>7</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>interior_alpha</code> <p>Opacity of the filled mask interior, in <code>[0, 1]</code>. Defaults to <code>0.3</code>.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.3</code> </p> <code>edge_alpha</code> <p>Opacity of the mask boundary, in <code>[0, 1]</code>. Defaults to <code>1.0</code>.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>edge_width</code> <p>Boundary thickness in pixels. Controls the erosion kernel size as <code>2 * edge_width + 1</code>. Defaults to <code>1</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>**imshow_kwargs</code> <p>Additional keyword arguments forwarded to <code>ax.imshow</code> for the DRR image only, not the mask.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>list[Axes]</code> <p>List of <code>Axes</code> of length <code>B</code>, one per image in the batch.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If <code>img</code> has more than one channel and <code>mask</code> is also provided.</p> Source code in <code>src/nanodrr/plot/plot.py</code> <pre><code>def plot_drr(\n    img: Float[torch.Tensor, \"B C H W\"],\n    mask: Bool[torch.Tensor, \"B C H W\"] | None = None,\n    title: list[str] | None = None,\n    ticks: bool = True,\n    axs: list[matplotlib.axes.Axes] | None = None,\n    cmap: str = \"gray\",\n    mask_cmap: str | matplotlib.colors.Colormap = \"Set2\",\n    mask_n_colors: int = 7,\n    interior_alpha: float = 0.3,\n    edge_alpha: float = 1.0,\n    edge_width: int = 1,\n    **imshow_kwargs,\n) -&gt; list[matplotlib.axes.Axes]:\n    \"\"\"Plot a batch of DRR images, optionally with a segmentation mask overlay.\n\n    Renders each image by summing across channels, simulating X-ray intensity\n    accumulation along a ray. A segmentation mask can be overlaid in two ways:\n    passed explicitly via ``mask``, or derived automatically when ``img`` has\n    more than one channel (where channel 0 is background and channels 1+ are\n    labeled structures). These two modes are mutually exclusive.\n\n    When a mask is rendered, channel 0 is always dropped. It is assumed to\n    represent background. Each remaining channel is drawn with a distinct\n    color, a translucent interior fill, and an opaque boundary edge detected\n    via morphological erosion.\n\n    Args:\n        img: Batch of DRR images with shape ``(B, C, H, W)``. If ``C &gt; 1``,\n            channels 1+ are treated as binary segmentation labels and a mask\n            is derived as ``img &gt; 0``. Channel intensities are summed across\n            ``C`` for display.\n        mask: Explicit segmentation mask with shape ``(B, C, H, W)``, where\n            channel 0 is background and channels 1+ are labeled structures.\n            Mutually exclusive with a multi-channel ``img``.\n        title: Per-image labels of length ``B``, rendered as x-axis titles.\n            If ``None``, no labels are shown.\n        ticks: Whether to display 1-indexed pixel coordinate ticks. If\n            ``False``, all tick marks are hidden. Defaults to ``True``.\n        axs: Pre-existing axes to plot into. Must have length ``B``. If\n            ``None``, a new figure with ``B`` subplots is created.\n        cmap: Colormap for the DRR image. Defaults to ``\"gray\"``.\n        mask_cmap: Colormap used to assign colors to segmentation channels.\n            Colors are sampled evenly and cycled if the number of channels\n            exceeds ``mask_n_colors``. Defaults to ``\"Set2\"``.\n        mask_n_colors: Number of evenly spaced colors to sample from\n            ``mask_cmap`` before cycling. Defaults to ``7``.\n        interior_alpha: Opacity of the filled mask interior, in ``[0, 1]``.\n            Defaults to ``0.3``.\n        edge_alpha: Opacity of the mask boundary, in ``[0, 1]``.\n            Defaults to ``1.0``.\n        edge_width: Boundary thickness in pixels. Controls the erosion kernel\n            size as ``2 * edge_width + 1``. Defaults to ``1``.\n        **imshow_kwargs: Additional keyword arguments forwarded to\n            ``ax.imshow`` for the DRR image only, not the mask.\n\n    Returns:\n        List of ``Axes`` of length ``B``, one per image in the batch.\n\n    Raises:\n        ValueError: If ``img`` has more than one channel and ``mask`` is\n            also provided.\n    \"\"\"\n    if img.shape[1] &gt; 1 and mask is not None:\n        raise ValueError(\"Pass either a multi-channel img or an explicit mask, not both.\")\n\n    axs = _plot_img(img.sum(dim=1, keepdim=True), title, ticks, axs, cmap, **imshow_kwargs)\n\n    if img.shape[1] &gt; 1:\n        mask = img &gt; 0\n    elif mask is None:\n        return axs\n\n    _plot_mask(\n        mask[:, 1:].float(),\n        axs=axs,\n        mask_cmap=mask_cmap,\n        mask_n_colors=mask_n_colors,\n        interior_alpha=interior_alpha,\n        edge_alpha=edge_alpha,\n        edge_width=edge_width,\n    )\n    return axs\n</code></pre>"},{"location":"reference/nanodrr/plot/plot/","title":"plot","text":""},{"location":"reference/nanodrr/plot/plot/#nanodrr.plot.plot","title":"nanodrr.plot.plot","text":""},{"location":"reference/nanodrr/plot/plot/#nanodrr.plot.plot.plot_drr","title":"plot_drr","text":"<pre><code>plot_drr(\n    img: Float[Tensor, \"B C H W\"],\n    mask: Bool[Tensor, \"B C H W\"] | None = None,\n    title: list[str] | None = None,\n    ticks: bool = True,\n    axs: list[Axes] | None = None,\n    cmap: str = \"gray\",\n    mask_cmap: str | Colormap = \"Set2\",\n    mask_n_colors: int = 7,\n    interior_alpha: float = 0.3,\n    edge_alpha: float = 1.0,\n    edge_width: int = 1,\n    **imshow_kwargs\n) -&gt; list[Axes]\n</code></pre> <p>Plot a batch of DRR images, optionally with a segmentation mask overlay.</p> <p>Renders each image by summing across channels, simulating X-ray intensity accumulation along a ray. A segmentation mask can be overlaid in two ways: passed explicitly via <code>mask</code>, or derived automatically when <code>img</code> has more than one channel (where channel 0 is background and channels 1+ are labeled structures). These two modes are mutually exclusive.</p> <p>When a mask is rendered, channel 0 is always dropped. It is assumed to represent background. Each remaining channel is drawn with a distinct color, a translucent interior fill, and an opaque boundary edge detected via morphological erosion.</p> PARAMETER DESCRIPTION <code>img</code> <p>Batch of DRR images with shape <code>(B, C, H, W)</code>. If <code>C &gt; 1</code>, channels 1+ are treated as binary segmentation labels and a mask is derived as <code>img &gt; 0</code>. Channel intensities are summed across <code>C</code> for display.</p> <p> TYPE: <code>Float[Tensor, 'B C H W']</code> </p> <code>mask</code> <p>Explicit segmentation mask with shape <code>(B, C, H, W)</code>, where channel 0 is background and channels 1+ are labeled structures. Mutually exclusive with a multi-channel <code>img</code>.</p> <p> TYPE: <code>Bool[Tensor, 'B C H W'] | None</code> DEFAULT: <code>None</code> </p> <code>title</code> <p>Per-image labels of length <code>B</code>, rendered as x-axis titles. If <code>None</code>, no labels are shown.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>ticks</code> <p>Whether to display 1-indexed pixel coordinate ticks. If <code>False</code>, all tick marks are hidden. Defaults to <code>True</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>axs</code> <p>Pre-existing axes to plot into. Must have length <code>B</code>. If <code>None</code>, a new figure with <code>B</code> subplots is created.</p> <p> TYPE: <code>list[Axes] | None</code> DEFAULT: <code>None</code> </p> <code>cmap</code> <p>Colormap for the DRR image. Defaults to <code>\"gray\"</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'gray'</code> </p> <code>mask_cmap</code> <p>Colormap used to assign colors to segmentation channels. Colors are sampled evenly and cycled if the number of channels exceeds <code>mask_n_colors</code>. Defaults to <code>\"Set2\"</code>.</p> <p> TYPE: <code>str | Colormap</code> DEFAULT: <code>'Set2'</code> </p> <code>mask_n_colors</code> <p>Number of evenly spaced colors to sample from <code>mask_cmap</code> before cycling. Defaults to <code>7</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>interior_alpha</code> <p>Opacity of the filled mask interior, in <code>[0, 1]</code>. Defaults to <code>0.3</code>.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.3</code> </p> <code>edge_alpha</code> <p>Opacity of the mask boundary, in <code>[0, 1]</code>. Defaults to <code>1.0</code>.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>edge_width</code> <p>Boundary thickness in pixels. Controls the erosion kernel size as <code>2 * edge_width + 1</code>. Defaults to <code>1</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>**imshow_kwargs</code> <p>Additional keyword arguments forwarded to <code>ax.imshow</code> for the DRR image only, not the mask.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>list[Axes]</code> <p>List of <code>Axes</code> of length <code>B</code>, one per image in the batch.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If <code>img</code> has more than one channel and <code>mask</code> is also provided.</p> Source code in <code>src/nanodrr/plot/plot.py</code> <pre><code>def plot_drr(\n    img: Float[torch.Tensor, \"B C H W\"],\n    mask: Bool[torch.Tensor, \"B C H W\"] | None = None,\n    title: list[str] | None = None,\n    ticks: bool = True,\n    axs: list[matplotlib.axes.Axes] | None = None,\n    cmap: str = \"gray\",\n    mask_cmap: str | matplotlib.colors.Colormap = \"Set2\",\n    mask_n_colors: int = 7,\n    interior_alpha: float = 0.3,\n    edge_alpha: float = 1.0,\n    edge_width: int = 1,\n    **imshow_kwargs,\n) -&gt; list[matplotlib.axes.Axes]:\n    \"\"\"Plot a batch of DRR images, optionally with a segmentation mask overlay.\n\n    Renders each image by summing across channels, simulating X-ray intensity\n    accumulation along a ray. A segmentation mask can be overlaid in two ways:\n    passed explicitly via ``mask``, or derived automatically when ``img`` has\n    more than one channel (where channel 0 is background and channels 1+ are\n    labeled structures). These two modes are mutually exclusive.\n\n    When a mask is rendered, channel 0 is always dropped. It is assumed to\n    represent background. Each remaining channel is drawn with a distinct\n    color, a translucent interior fill, and an opaque boundary edge detected\n    via morphological erosion.\n\n    Args:\n        img: Batch of DRR images with shape ``(B, C, H, W)``. If ``C &gt; 1``,\n            channels 1+ are treated as binary segmentation labels and a mask\n            is derived as ``img &gt; 0``. Channel intensities are summed across\n            ``C`` for display.\n        mask: Explicit segmentation mask with shape ``(B, C, H, W)``, where\n            channel 0 is background and channels 1+ are labeled structures.\n            Mutually exclusive with a multi-channel ``img``.\n        title: Per-image labels of length ``B``, rendered as x-axis titles.\n            If ``None``, no labels are shown.\n        ticks: Whether to display 1-indexed pixel coordinate ticks. If\n            ``False``, all tick marks are hidden. Defaults to ``True``.\n        axs: Pre-existing axes to plot into. Must have length ``B``. If\n            ``None``, a new figure with ``B`` subplots is created.\n        cmap: Colormap for the DRR image. Defaults to ``\"gray\"``.\n        mask_cmap: Colormap used to assign colors to segmentation channels.\n            Colors are sampled evenly and cycled if the number of channels\n            exceeds ``mask_n_colors``. Defaults to ``\"Set2\"``.\n        mask_n_colors: Number of evenly spaced colors to sample from\n            ``mask_cmap`` before cycling. Defaults to ``7``.\n        interior_alpha: Opacity of the filled mask interior, in ``[0, 1]``.\n            Defaults to ``0.3``.\n        edge_alpha: Opacity of the mask boundary, in ``[0, 1]``.\n            Defaults to ``1.0``.\n        edge_width: Boundary thickness in pixels. Controls the erosion kernel\n            size as ``2 * edge_width + 1``. Defaults to ``1``.\n        **imshow_kwargs: Additional keyword arguments forwarded to\n            ``ax.imshow`` for the DRR image only, not the mask.\n\n    Returns:\n        List of ``Axes`` of length ``B``, one per image in the batch.\n\n    Raises:\n        ValueError: If ``img`` has more than one channel and ``mask`` is\n            also provided.\n    \"\"\"\n    if img.shape[1] &gt; 1 and mask is not None:\n        raise ValueError(\"Pass either a multi-channel img or an explicit mask, not both.\")\n\n    axs = _plot_img(img.sum(dim=1, keepdim=True), title, ticks, axs, cmap, **imshow_kwargs)\n\n    if img.shape[1] &gt; 1:\n        mask = img &gt; 0\n    elif mask is None:\n        return axs\n\n    _plot_mask(\n        mask[:, 1:].float(),\n        axs=axs,\n        mask_cmap=mask_cmap,\n        mask_n_colors=mask_n_colors,\n        interior_alpha=interior_alpha,\n        edge_alpha=edge_alpha,\n        edge_width=edge_width,\n    )\n    return axs\n</code></pre>"},{"location":"reference/nanodrr/registration/","title":"registration","text":""},{"location":"reference/nanodrr/registration/#nanodrr.registration","title":"nanodrr.registration","text":""},{"location":"reference/nanodrr/registration/#nanodrr.registration.Registration","title":"Registration","text":"<pre><code>Registration(\n    subject: Subject,\n    rt_inv: Float[Tensor, \"B 4 4\"],\n    k_inv: Float[Tensor, \"B 3 3\"],\n    sdd: Float[Tensor, B],\n    height: int,\n    width: int,\n    eps: float = 1e-08,\n)\n</code></pre> <p>Differentiable 2D/3D registration module.</p> <p>Optimize poses in SE(3) by aligning rendered X-rays with real target X-ray images. Initial intrinsic and extrinsic matrices are fixed at construction. Optimizable parameters are parameterized as perturbations.</p> PARAMETER DESCRIPTION <code>subject</code> <p>The volume to render DRRs from during optimization.</p> <p> TYPE: <code>Subject</code> </p> <code>rt_inv</code> <p>Initial inverse extrinsic (camera-to-world) matrix.</p> <p> TYPE: <code>Float[Tensor, 'B 4 4']</code> </p> <code>k_inv</code> <p>Inverse intrinsic camera matrix.</p> <p> TYPE: <code>Float[Tensor, 'B 3 3']</code> </p> <code>sdd</code> <p>Source-to-detector distance.</p> <p> TYPE: <code>Float[Tensor, B]</code> </p> <code>height</code> <p>Output image height in pixels.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Output image width in pixels.</p> <p> TYPE: <code>int</code> </p> <code>eps</code> <p>Small constant for numerical stability.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-08</code> </p> Source code in <code>src/nanodrr/registration/registration.py</code> <pre><code>def __init__(\n    self,\n    subject: Subject,\n    rt_inv: Float[torch.Tensor, \"B 4 4\"],\n    k_inv: Float[torch.Tensor, \"B 3 3\"],\n    sdd: Float[torch.Tensor, \"B\"],\n    height: int,\n    width: int,\n    eps: float = 1e-8,\n):\n    super().__init__()\n    self.subject = subject\n    self.rt_inv = rt_inv\n    self.k_inv = k_inv\n    self.sdd = sdd\n    self.height = height\n    self.width = width\n\n    # Initialize the perturbations\n    B = len(self.rt_inv)\n    self._rot = torch.nn.Parameter(eps * torch.randn(B, 3, device=c.device))\n    self._xyz = torch.nn.Parameter(eps * torch.randn(B, 3, device=c.device))\n</code></pre>"},{"location":"reference/nanodrr/registration/registration/","title":"registration","text":""},{"location":"reference/nanodrr/registration/registration/#nanodrr.registration.registration","title":"nanodrr.registration.registration","text":""},{"location":"reference/nanodrr/registration/registration/#nanodrr.registration.registration.Registration","title":"Registration","text":"<pre><code>Registration(\n    subject: Subject,\n    rt_inv: Float[Tensor, \"B 4 4\"],\n    k_inv: Float[Tensor, \"B 3 3\"],\n    sdd: Float[Tensor, B],\n    height: int,\n    width: int,\n    eps: float = 1e-08,\n)\n</code></pre> <p>Differentiable 2D/3D registration module.</p> <p>Optimize poses in SE(3) by aligning rendered X-rays with real target X-ray images. Initial intrinsic and extrinsic matrices are fixed at construction. Optimizable parameters are parameterized as perturbations.</p> PARAMETER DESCRIPTION <code>subject</code> <p>The volume to render DRRs from during optimization.</p> <p> TYPE: <code>Subject</code> </p> <code>rt_inv</code> <p>Initial inverse extrinsic (camera-to-world) matrix.</p> <p> TYPE: <code>Float[Tensor, 'B 4 4']</code> </p> <code>k_inv</code> <p>Inverse intrinsic camera matrix.</p> <p> TYPE: <code>Float[Tensor, 'B 3 3']</code> </p> <code>sdd</code> <p>Source-to-detector distance.</p> <p> TYPE: <code>Float[Tensor, B]</code> </p> <code>height</code> <p>Output image height in pixels.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Output image width in pixels.</p> <p> TYPE: <code>int</code> </p> <code>eps</code> <p>Small constant for numerical stability.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-08</code> </p> Source code in <code>src/nanodrr/registration/registration.py</code> <pre><code>def __init__(\n    self,\n    subject: Subject,\n    rt_inv: Float[torch.Tensor, \"B 4 4\"],\n    k_inv: Float[torch.Tensor, \"B 3 3\"],\n    sdd: Float[torch.Tensor, \"B\"],\n    height: int,\n    width: int,\n    eps: float = 1e-8,\n):\n    super().__init__()\n    self.subject = subject\n    self.rt_inv = rt_inv\n    self.k_inv = k_inv\n    self.sdd = sdd\n    self.height = height\n    self.width = width\n\n    # Initialize the perturbations\n    B = len(self.rt_inv)\n    self._rot = torch.nn.Parameter(eps * torch.randn(B, 3, device=c.device))\n    self._xyz = torch.nn.Parameter(eps * torch.randn(B, 3, device=c.device))\n</code></pre>"},{"location":"reference/nanodrr/scene/","title":"scene","text":""},{"location":"reference/nanodrr/scene/#nanodrr.scene","title":"nanodrr.scene","text":""},{"location":"reference/nanodrr/scene/#nanodrr.scene.visualize_scene","title":"visualize_scene","text":"<pre><code>visualize_scene(\n    subject: Subject,\n    k_inv: Float[Tensor, \"B 3 3\"],\n    rt_inv: Float[Tensor, \"B 4 4\"],\n    sdd: Float[Tensor, B],\n    height: int,\n    width: int,\n    render_imgs: bool = True,\n    single_channel: bool = False,\n    culling: str | None = \"back\",\n    verbose: bool = False,\n    **kwargs\n) -&gt; Plotter\n</code></pre> <p>Render a DRR and return a 3D scene with camera frustums and anatomy.</p> PARAMETER DESCRIPTION <code>subject</code> <p>The subject containing the CT volume and labelmap.</p> <p> TYPE: <code>Subject</code> </p> <code>k_inv</code> <p>Inverse intrinsic matrices.</p> <p> TYPE: <code>Float[Tensor, 'B 3 3']</code> </p> <code>rt_inv</code> <p>Camera-to-world transforms.</p> <p> TYPE: <code>Float[Tensor, 'B 4 4']</code> </p> <code>sdd</code> <p>Source-to-detector distances.</p> <p> TYPE: <code>Float[Tensor, B]</code> </p> <code>height</code> <p>Detector height in pixels.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Detector width in pixels.</p> <p> TYPE: <code>int</code> </p> <code>render_imgs</code> <p>If True, render DRRs before plotting</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>single_channel</code> <p>If True, sum channels before texturing the detector.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>culling</code> <p>Face culling mode passed to each mesh (e.g. <code>\"back\"</code>).</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'back'</code> </p> <code>verbose</code> <p>If True, print progress during mesh extraction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Additional arguments forwarded to :func:<code>render</code>.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Plotter</code> <p>A PyVista plotter with the anatomy mesh and camera frustums added.</p> Source code in <code>src/nanodrr/scene/scene.py</code> <pre><code>def visualize_scene(\n    subject: Subject,\n    k_inv: Float[torch.Tensor, \"B 3 3\"],\n    rt_inv: Float[torch.Tensor, \"B 4 4\"],\n    sdd: Float[torch.Tensor, \"B\"],\n    height: int,\n    width: int,\n    render_imgs: bool = True,\n    single_channel: bool = False,\n    culling: str | None = \"back\",\n    verbose: bool = False,\n    **kwargs,\n) -&gt; pv.Plotter:\n    \"\"\"Render a DRR and return a 3D scene with camera frustums and anatomy.\n\n    Args:\n        subject: The subject containing the CT volume and labelmap.\n        k_inv: Inverse intrinsic matrices.\n        rt_inv: Camera-to-world transforms.\n        sdd: Source-to-detector distances.\n        height: Detector height in pixels.\n        width: Detector width in pixels.\n        render_imgs: If True, render DRRs before plotting\n        single_channel: If True, sum channels before texturing the detector.\n        culling: Face culling mode passed to each mesh (e.g. ``\"back\"``).\n        verbose: If True, print progress during mesh extraction.\n        **kwargs: Additional arguments forwarded to :func:`render`.\n\n    Returns:\n        A PyVista plotter with the anatomy mesh and camera frustums added.\n    \"\"\"\n    # Get a mesh from the subject's labelmap\n    mesh = label_to_mesh(subject, verbose)\n\n    # Render the DRR\n    img = None\n    if render_imgs:\n        img = render(subject, k_inv, rt_inv, sdd, height, width, **kwargs)\n        img = img.sum(dim=1, keepdim=True) if single_channel else img\n\n    # Make the cameras\n    cameras = make_cameras(k_inv, rt_inv, sdd, height, width, img)\n\n    # Make the scene\n    pl = pv.Plotter()\n    pl.add_mesh(mesh)\n    for cam in cameras:\n        if render_imgs:\n            pl.add_mesh(cam[\"detector\"], texture=cam[\"texture\"], lighting=False, culling=culling)\n        pl.add_mesh(cam[\"camera\"], show_edges=True, line_width=3, culling=culling)\n        pl.add_mesh(cam[\"principal_ray\"], line_width=3, color=\"lime\", culling=culling)\n    return pl\n</code></pre>"},{"location":"reference/nanodrr/scene/camera/","title":"camera","text":""},{"location":"reference/nanodrr/scene/camera/#nanodrr.scene.camera","title":"nanodrr.scene.camera","text":""},{"location":"reference/nanodrr/scene/camera/#nanodrr.scene.camera.make_cameras","title":"make_cameras","text":"<pre><code>make_cameras(\n    k_inv: Float[Tensor, \"B 3 3\"],\n    rt_inv: Float[Tensor, \"B 4 4\"],\n    sdd: Float[Tensor, B],\n    height: int,\n    width: int,\n    img: Float[Tensor, \"B C H W\"] | None = None,\n    frustum_size: float = 0.125,\n) -&gt; list[dict]\n</code></pre> <p>Build PyVista meshes for visualizing cameras in a 3D scene.</p> <p>For each camera in the batch, computes the source (X-ray point source) and detector plane positions in world coordinates, then constructs meshes for the frustum, principal ray, and optionally the detector surface with a projected image texture.</p> PARAMETER DESCRIPTION <code>k_inv</code> <p>Inverse intrinsic camera matrix.</p> <p> TYPE: <code>Float[Tensor, 'B 3 3']</code> </p> <code>rt_inv</code> <p>Inverse extrinsic (camera-to-world) matrix.</p> <p> TYPE: <code>Float[Tensor, 'B 4 4']</code> </p> <code>sdd</code> <p>Source-to-detector distance.</p> <p> TYPE: <code>Float[Tensor, B]</code> </p> <code>height</code> <p>Detector image height in pixels.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Detector image width in pixels.</p> <p> TYPE: <code>int</code> </p> <code>img</code> <p>Optional rendered or target image to texture onto the detector plane. If <code>None</code>, detector and texture entries are omitted.</p> <p> TYPE: <code>Float[Tensor, 'B C H W'] | None</code> DEFAULT: <code>None</code> </p> <code>frustum_size</code> <p>Scale factor for the camera frustum wireframe.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.125</code> </p> RETURNS DESCRIPTION <code>list[dict]</code> <p>List of dicts, one per camera in the batch. Each dict contains:</p> <ul> <li><code>\"detector\"</code>: PyVista mesh of the detector plane, or <code>None</code>.</li> <li><code>\"texture\"</code>: PyVista texture from <code>img</code>, or <code>None</code>.</li> <li><code>\"camera\"</code>: PyVista mesh of the camera frustum wireframe.</li> <li><code>\"principal_ray\"</code>: PyVista <code>Line</code> from source to detector center.</li> </ul> Source code in <code>src/nanodrr/scene/camera.py</code> <pre><code>def make_cameras(\n    k_inv: Float[torch.Tensor, \"B 3 3\"],\n    rt_inv: Float[torch.Tensor, \"B 4 4\"],\n    sdd: Float[torch.Tensor, \"B\"],\n    height: int,\n    width: int,\n    img: Float[torch.Tensor, \"B C H W\"] | None = None,\n    frustum_size: float = 0.125,\n) -&gt; list[dict]:\n    \"\"\"Build PyVista meshes for visualizing cameras in a 3D scene.\n\n    For each camera in the batch, computes the source (X-ray point source)\n    and detector plane positions in world coordinates, then constructs\n    meshes for the frustum, principal ray, and optionally the detector\n    surface with a projected image texture.\n\n    Args:\n        k_inv: Inverse intrinsic camera matrix.\n        rt_inv: Inverse extrinsic (camera-to-world) matrix.\n        sdd: Source-to-detector distance.\n        height: Detector image height in pixels.\n        width: Detector image width in pixels.\n        img: Optional rendered or target image to texture onto the detector\n            plane. If `None`, detector and texture entries are omitted.\n        frustum_size: Scale factor for the camera frustum wireframe.\n\n    Returns:\n        List of dicts, one per camera in the batch. Each dict contains:\n\n            - `\"detector\"`: PyVista mesh of the detector plane, or `None`.\n            - `\"texture\"`: PyVista texture from `img`, or `None`.\n            - `\"camera\"`: PyVista mesh of the camera frustum wireframe.\n            - `\"principal_ray\"`: PyVista `Line` from source to detector center.\n    \"\"\"\n    B = rt_inv.shape[0]\n    device, dtype = rt_inv.device, rt_inv.dtype\n\n    src_cam = torch.zeros(B, 1, 3, device=device, dtype=dtype)\n    tgt_cam = _make_tgt(k_inv, sdd, height, width, device, dtype)\n\n    src = transform_point(rt_inv, src_cam).squeeze(1).cpu().detach().numpy()\n    tgt = transform_point(rt_inv, tgt_cam).reshape(B, height, width, 3).cpu().detach().numpy()\n\n    return [\n        {\n            \"detector\": _detector(tgt[b]) if img is not None else None,\n            \"texture\": _texture(img[b : b + 1]) if img is not None else None,\n            \"camera\": _frustum(src[b], tgt[b], frustum_size),\n            \"principal_ray\": pv.Line(src[b], tgt[b].mean(axis=(0, 1))),\n        }\n        for b in range(B)\n    ]\n</code></pre>"},{"location":"reference/nanodrr/scene/scene/","title":"scene","text":""},{"location":"reference/nanodrr/scene/scene/#nanodrr.scene.scene","title":"nanodrr.scene.scene","text":""},{"location":"reference/nanodrr/scene/scene/#nanodrr.scene.scene.visualize_scene","title":"visualize_scene","text":"<pre><code>visualize_scene(\n    subject: Subject,\n    k_inv: Float[Tensor, \"B 3 3\"],\n    rt_inv: Float[Tensor, \"B 4 4\"],\n    sdd: Float[Tensor, B],\n    height: int,\n    width: int,\n    render_imgs: bool = True,\n    single_channel: bool = False,\n    culling: str | None = \"back\",\n    verbose: bool = False,\n    **kwargs\n) -&gt; Plotter\n</code></pre> <p>Render a DRR and return a 3D scene with camera frustums and anatomy.</p> PARAMETER DESCRIPTION <code>subject</code> <p>The subject containing the CT volume and labelmap.</p> <p> TYPE: <code>Subject</code> </p> <code>k_inv</code> <p>Inverse intrinsic matrices.</p> <p> TYPE: <code>Float[Tensor, 'B 3 3']</code> </p> <code>rt_inv</code> <p>Camera-to-world transforms.</p> <p> TYPE: <code>Float[Tensor, 'B 4 4']</code> </p> <code>sdd</code> <p>Source-to-detector distances.</p> <p> TYPE: <code>Float[Tensor, B]</code> </p> <code>height</code> <p>Detector height in pixels.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Detector width in pixels.</p> <p> TYPE: <code>int</code> </p> <code>render_imgs</code> <p>If True, render DRRs before plotting</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>single_channel</code> <p>If True, sum channels before texturing the detector.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>culling</code> <p>Face culling mode passed to each mesh (e.g. <code>\"back\"</code>).</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'back'</code> </p> <code>verbose</code> <p>If True, print progress during mesh extraction.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>Additional arguments forwarded to :func:<code>render</code>.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Plotter</code> <p>A PyVista plotter with the anatomy mesh and camera frustums added.</p> Source code in <code>src/nanodrr/scene/scene.py</code> <pre><code>def visualize_scene(\n    subject: Subject,\n    k_inv: Float[torch.Tensor, \"B 3 3\"],\n    rt_inv: Float[torch.Tensor, \"B 4 4\"],\n    sdd: Float[torch.Tensor, \"B\"],\n    height: int,\n    width: int,\n    render_imgs: bool = True,\n    single_channel: bool = False,\n    culling: str | None = \"back\",\n    verbose: bool = False,\n    **kwargs,\n) -&gt; pv.Plotter:\n    \"\"\"Render a DRR and return a 3D scene with camera frustums and anatomy.\n\n    Args:\n        subject: The subject containing the CT volume and labelmap.\n        k_inv: Inverse intrinsic matrices.\n        rt_inv: Camera-to-world transforms.\n        sdd: Source-to-detector distances.\n        height: Detector height in pixels.\n        width: Detector width in pixels.\n        render_imgs: If True, render DRRs before plotting\n        single_channel: If True, sum channels before texturing the detector.\n        culling: Face culling mode passed to each mesh (e.g. ``\"back\"``).\n        verbose: If True, print progress during mesh extraction.\n        **kwargs: Additional arguments forwarded to :func:`render`.\n\n    Returns:\n        A PyVista plotter with the anatomy mesh and camera frustums added.\n    \"\"\"\n    # Get a mesh from the subject's labelmap\n    mesh = label_to_mesh(subject, verbose)\n\n    # Render the DRR\n    img = None\n    if render_imgs:\n        img = render(subject, k_inv, rt_inv, sdd, height, width, **kwargs)\n        img = img.sum(dim=1, keepdim=True) if single_channel else img\n\n    # Make the cameras\n    cameras = make_cameras(k_inv, rt_inv, sdd, height, width, img)\n\n    # Make the scene\n    pl = pv.Plotter()\n    pl.add_mesh(mesh)\n    for cam in cameras:\n        if render_imgs:\n            pl.add_mesh(cam[\"detector\"], texture=cam[\"texture\"], lighting=False, culling=culling)\n        pl.add_mesh(cam[\"camera\"], show_edges=True, line_width=3, culling=culling)\n        pl.add_mesh(cam[\"principal_ray\"], line_width=3, color=\"lime\", culling=culling)\n    return pl\n</code></pre>"},{"location":"reference/nanodrr/scene/surface/","title":"surface","text":""},{"location":"reference/nanodrr/scene/surface/#nanodrr.scene.surface","title":"nanodrr.scene.surface","text":""},{"location":"reference/nanodrr/scene/surface/#nanodrr.scene.surface.label_to_mesh","title":"label_to_mesh","text":"<pre><code>label_to_mesh(subject: Subject, verbose: bool = False) -&gt; PolyData\n</code></pre> <p>Convert a subject's labelmap into a PyVista surface mesh.</p> <p>Extracts isosurfaces for each label class using marching cubes and optionally flips face normals to ensure consistent outward orientation.</p> PARAMETER DESCRIPTION <code>subject</code> <p>Subject containing a multi-class labelmap in <code>subject.label</code>.</p> <p> TYPE: <code>Subject</code> </p> <code>verbose</code> <p>If <code>True</code>, display a progress bar during meshing.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>PolyData</code> <p>Surface mesh with one connected region per label class.</p> Source code in <code>src/nanodrr/scene/surface.py</code> <pre><code>def label_to_mesh(subject: Subject, verbose: bool = False) -&gt; pv.PolyData:\n    \"\"\"Convert a subject's labelmap into a PyVista surface mesh.\n\n    Extracts isosurfaces for each label class using marching cubes and\n    optionally flips face normals to ensure consistent outward orientation.\n\n    Args:\n        subject: Subject containing a multi-class labelmap in `subject.label`.\n        verbose: If `True`, display a progress bar during meshing.\n\n    Returns:\n        Surface mesh with one connected region per label class.\n    \"\"\"\n    label, invert = subject_to_imagedata(subject, use_label=True)\n    mesh = label.contour_labels(progress_bar=verbose)\n    if invert:\n        mesh = mesh.flip_faces(progress_bar=verbose)\n    return mesh\n</code></pre>"},{"location":"reference/nanodrr/scene/utils/","title":"utils","text":""},{"location":"reference/nanodrr/scene/utils/#nanodrr.scene.utils","title":"nanodrr.scene.utils","text":""},{"location":"reference/nanodrr/scene/utils/#nanodrr.scene.utils.subject_to_imagedata","title":"subject_to_imagedata","text":"<pre><code>subject_to_imagedata(\n    subject: Subject, use_label: bool = False\n) -&gt; tuple[ImageData, bool]\n</code></pre> <p>Convert a subject's volume or labelmap into a PyVista <code>ImageData</code> grid.</p> <p>Reorders the volume axes for VTK convention, applies the voxel-to-world affine transform, and detects whether the affine has a negative determinant (indicating a left-handed coordinate system that requires face flipping for correct surface normals).</p> PARAMETER DESCRIPTION <code>subject</code> <p>Subject containing <code>subject.image</code> and optionally <code>subject.label</code>.</p> <p> TYPE: <code>Subject</code> </p> <code>use_label</code> <p>If <code>True</code>, convert the labelmap instead of the density volume.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple[ImageData, bool]</code> <p>A tuple of: - The transformed <code>ImageData</code> grid in world coordinates. - <code>True</code> if the affine has a negative determinant (faces should   be flipped), <code>False</code> otherwise.</p> Source code in <code>src/nanodrr/scene/utils.py</code> <pre><code>def subject_to_imagedata(\n    subject: Subject,\n    use_label: bool = False,\n) -&gt; tuple[pv.ImageData, bool]:\n    \"\"\"Convert a subject's volume or labelmap into a PyVista `ImageData` grid.\n\n    Reorders the volume axes for VTK convention, applies the voxel-to-world\n    affine transform, and detects whether the affine has a negative\n    determinant (indicating a left-handed coordinate system that requires\n    face flipping for correct surface normals).\n\n    Args:\n        subject: Subject containing `subject.image` and optionally\n            `subject.label`.\n        use_label: If `True`, convert the labelmap instead of the density\n            volume.\n\n    Returns:\n        A tuple of:\n            - The transformed `ImageData` grid in world coordinates.\n            - `True` if the affine has a negative determinant (faces should\n              be flipped), `False` otherwise.\n    \"\"\"\n    data = subject.label if use_label else subject.image\n    data = data.squeeze().cpu().permute(2, 1, 0).numpy()\n    affine = subject.voxel_to_world.cpu().numpy()\n    invert = np.linalg.det(affine) &lt; 0\n\n    grid = pv.ImageData(\n        dimensions=data.shape,\n        spacing=(1, 1, 1),\n        origin=(0, 0, 0),\n    )\n    grid.point_data[\"values\"] = data.flatten(order=\"F\")\n    return grid.transform(affine, inplace=False), invert\n</code></pre>"}]}